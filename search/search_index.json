{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"GPTDataset/","text":"GPTDataset \u51fd\u6570\u5730\u5740\uff1a https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/gpt_dataset.py#L139-L187 \u51fd\u6570\u529f\u80fd\uff1a \u521b\u5efa\u9002\u7528\u4e8eGPT\u6a21\u578b\u7684dataset\u3002 class GPTDataset ( torch . utils . data . Dataset ): def __init__ ( self , name , data_prefix , documents , indexed_dataset , num_samples , seq_length , seed ): self . name = name self . indexed_dataset = indexed_dataset # Checks assert np . min ( documents ) >= 0 assert np . max ( documents ) < indexed_dataset . sizes . shape [ 0 ] # Build index mappings. # doc_idx\u662findexed_dataset\u6240\u6709document\u590d\u5236num_epochs\u904d\u4e4b\u540e\u6253\u4e71\u7684\u7ed3\u679c\uff0cshape=(len(document) * num_epochs, ) # sample_idx\u662f[doc_idx_index, offset]\u7684list\uff0cshape=(num_samples+1, 2)\uff0c\u6bcf\u4e24\u884c\u4e4b\u95f4\u662f\u4e00\u4e2asample # \u8fd9\u91cc\u591a\u8bf4\u4e00\u53e5\uff0csample_idx\u662f\u4e2a\u533a\u95f4\u7aef\u70b9\u7684idx\uff0c\u6240\u4ee5\u8981\u53d6\u4e00\u4e2asample\u9700\u8981\u7528\u4e24\u4e2a\u5143\u7d20 # \u6bd4\u5982\u53d6\u7b2c3\u4e2asample\uff08\u4ece0\u5f00\u59cb\u8ba1\u6570\uff09\uff0c\u5e94\u8be5\u7528sample_idx[3]\u548csample_idx[4]\u6765\u53d6\uff0c # \u5982\u679csample_idx[3] = [4, 10], sample_idx[4] = [5, 2] # \u90a3\u4e48\u7b2c\u4e09\u4e2asample\u5c31\u662findexed_dataest\u4e2d\u7b2c4\u4e2a\u6587\u6863\u7b2c10\u4e2a\u8bcd\u5f00\u59cb\uff0c\u5230\u7b2c5\u4e2a\u6587\u6863\u7b2c2\u4e2a\u8bcd\u7ed3\u675f\uff08\u4e0d\u5305\u542b\u7b2c2\u4e2a\u8bcd\uff09 # shuffle_idx\u5e94\u8be5\u662fshape=(sample_idx.shape[0], )\uff0c\u5355\u7eaf\u7528\u4e8e\u6253\u4e71 self . doc_idx , self . sample_idx , self . shuffle_idx = _build_index_mappings ( self . name , data_prefix , documents , self . indexed_dataset . sizes , num_samples , seq_length , seed ) def __len__ ( self ): # -1 is due to data structure used to retieve the index: # sample i --> [sample_idx[i], sample_idx[i+1]) return self . sample_idx . shape [ 0 ] - 1 def __getitem__ ( self , idx ): # Get the shuffled index. idx = self . shuffle_idx [ idx ] # Start and end documents and offsets. doc_index_f = self . sample_idx [ idx ][ 0 ] doc_index_l = self . sample_idx [ idx + 1 ][ 0 ] offset_f = self . sample_idx [ idx ][ 1 ] offset_l = self . sample_idx [ idx + 1 ][ 1 ] # If we are within the same document, just extract the chunk. # \u8fd9\u91cc\u5176\u5b9e\u6ca1\u770b\u61c2length\u4e3a\u4ec0\u4e48\u8981+1 if doc_index_f == doc_index_l : sample = self . indexed_dataset . get ( self . doc_idx [ doc_index_f ], offset = offset_f , length = offset_l - offset_f + 1 ) else : # Otherwise, get the rest of the initial document. sample_list = [ self . indexed_dataset . get ( self . doc_idx [ doc_index_f ], offset = offset_f )] # Loop over all in between documents and add the entire document. for i in range ( doc_index_f + 1 , doc_index_l ): sample_list . append ( self . indexed_dataset . get ( self . doc_idx [ i ])) # And finally add the relevant portion of last document. sample_list . append ( self . indexed_dataset . get ( self . doc_idx [ doc_index_l ], length = offset_l + 1 )) sample = np . concatenate ( sample_list ) return { 'text' : np . array ( sample , dtype = np . int64 )} build_sample_idx \u51fd\u6570\u5730\u5740\uff1a https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/helpers.cpp#L99-L185 \u51fd\u6570\u529f\u80fd\uff1a \u628aindexed_dataset\u7684\u6240\u6709document\u94fa\u5e73\uff0c\u6309\u7167\u7ed9\u5b9a\u7684seq_length\u5207\u5f00\uff0c\u751f\u6210\u4e0d\u540c\u7684\u8bad\u7ec3sample\uff0c\u8f93\u51fa\u7684sample_idx\u662f\u4e00\u4e2an*2\u7684\u6570\u7ec4\u3002\u6bcf\u4e00\u884c\u7684\u7b2c\u4e00\u5217\u662f\u8be5sample\u5bf9\u5e94\u7684doc_idx\uff0c\u7b2c\u4e8c\u5217\u662f\u8be5sample\u5f00\u5934\u4f4d\u7f6e\u5728\u8be5doc_idx\u4e0bdoc\u5bf9\u5e94\u7684offset\u3002 py :: array build_sample_idx ( const py :: array_t < int32_t > & sizes_ , const py :: array_t < int32_t > & doc_idx_ , const int32_t seq_length , const int32_t num_epochs , const int64_t tokens_per_epoch ) { /* Sample index (sample_idx) is used for gpt2 like dataset for which the documents are flattened and the samples are built based on this 1-D flatten array. It is a 2D array with sizes [number-of-samples + 1, 2] where [..., 0] contains the index into `doc_idx` and [..., 1] is the starting offset in that document.*/ // \u8fd9\u91cc\u8f93\u5165sizes_\u53c2\u6570\u662f\u6bcf\u4e2asentence\u7684\u957f\u5ea6\uff0cdoc_idx_\u662f\u6bcf\u4e2adoc\u5f00\u5934\u7684\u7d22\u5f15 // Consistency checks. assert ( seq_length > 1 ); assert ( num_epochs > 0 ); assert ( tokens_per_epoch > 1 ); // Remove bound checks. auto sizes = sizes_ . unchecked < 1 > (); auto doc_idx = doc_idx_ . unchecked < 1 > (); // Mapping and it's length (1D). int64_t num_samples = ( num_epochs * tokens_per_epoch - 1 ) / seq_length ; int32_t * sample_idx = new int32_t [ 2 * ( num_samples + 1 )]; cout << \" using:\" << endl << std :: flush ; cout << \" number of documents: \" << doc_idx_ . shape ( 0 ) / num_epochs << endl << std :: flush ; cout << \" number of epochs: \" << num_epochs << endl << std :: flush ; cout << \" sequence length: \" << seq_length << endl << std :: flush ; cout << \" total number of samples: \" << num_samples << endl << std :: flush ; // Index into sample_idx. int64_t sample_index = 0 ; // Index into doc_idx. int64_t doc_idx_index = 0 ; // Begining offset for each document. int32_t doc_offset = 0 ; // Start with first document and no offset. sample_idx [ 2 * sample_index ] = doc_idx_index ; sample_idx [ 2 * sample_index + 1 ] = doc_offset ; ++ sample_index ; while ( sample_index <= num_samples ) { // Start with a fresh sequence. int32_t remaining_seq_length = seq_length + 1 ; while ( remaining_seq_length != 0 ) { // Get the document length. auto doc_id = doc_idx [ doc_idx_index ]; auto doc_length = sizes [ doc_id ] - doc_offset ; // And add it to the current sequence. remaining_seq_length -= doc_length ; // If we have more than a full sequence, adjust offset and set // remaining length to zero so we return from the while loop. // Note that -1 here is for the same reason we have -1 in // `_num_epochs` calculations. // \u8fd9\u91cc\u5f53remaining_seq_length <= 0\u7684\u65f6\u5019\uff0cdoc_idx_index\u4e0d\u53d8\uff0c\u4f46\u662foffset\u8981\u589e\u52a0\u5230\u5f53\u524ddoc\u7684\u6700\u540e\u4e00\u4e2atoken\uff08why\uff09 if ( remaining_seq_length <= 0 ) { doc_offset += ( remaining_seq_length + doc_length - 1 ); remaining_seq_length = 0 ; } else { // Otherwise, start from the begining of the next document. ++ doc_idx_index ; doc_offset = 0 ; } } // Record the sequence. sample_idx [ 2 * sample_index ] = doc_idx_index ; sample_idx [ 2 * sample_index + 1 ] = doc_offset ; ++ sample_index ; } // Method to deallocate memory. py :: capsule free_when_done ( sample_idx , []( void * mem_ ) { int32_t * mem = reinterpret_cast < int32_t *> ( mem_ ); delete [] mem ; }); // Return the numpy array. const auto byte_size = sizeof ( int32_t ); return py :: array ( std :: vector < int64_t > { num_samples + 1 , 2 }, // shape { 2 * byte_size , byte_size }, // C-style contiguous strides sample_idx , // the data pointer free_when_done ); // numpy array references } _build_doc_idx \u51fd\u6570\u5730\u5740\uff1a https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/gpt_dataset.py#L346-L359 \u51fd\u6570\u529f\u80fd\uff1a \u628adocuments. (np.ndarray) \u590d\u5236num_epochs\u4efd\uff0c\u7136\u540eshuffle\uff0c\u867d\u7136\u6211\u4e0d\u77e5\u9053\u4e3a\u4ec0\u4e48\u8fd9\u4e48\u505a\u3002\u6253\u4e71\u4e4b\u540e\uff0c\u540c\u4e00\u4e2adoc\u53ef\u80fd\u5728\u4e00\u4e2aepoch\u8bad\u7ec3\u591a\u6b21\u3002 def _build_doc_idx ( documents , num_epochs , np_rng , separate_last_epoch ): \"\"\"Build an array with length = number-of-epochs * number-of-dcuments. Each index is mapped to a corresponding document.\"\"\" if not separate_last_epoch or num_epochs == 1 : # doc_idx\u662f\u4e00\u4e2a2\u7ef4np.ndarray\uff0c\u4e00\u5171num_epochs\u884c\uff0c\u6bcf\u4e00\u884c\u662fnp.arange(len(documents)) doc_idx = np . mgrid [ 0 : num_epochs , 0 : len ( documents )][ 1 ] doc_idx [:] = documents # flatten\u4e4b\u540e\u518d\u6253\u4e71\uff0c\u6bd4\u5982[[1, 2, 3], [1, 2, 3]]\u53d8\u6210[2, 3, 2, 1, 3, 1] doc_idx = doc_idx . reshape ( - 1 ) doc_idx = doc_idx . astype ( np . int32 ) np_rng . shuffle ( doc_idx ) return doc_idx doc_idx_first = _build_doc_idx ( documents , num_epochs - 1 , np_rng , False ) doc_idx_last = _build_doc_idx ( documents , 1 , np_rng , False ) return np . concatenate (( doc_idx_first , doc_idx_last )) _build_index_mappings \u51fd\u6570\u5730\u5740\uff1a https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/gpt_dataset.py#L190-L323 \u51fd\u6570\u529f\u80fd\uff1a \u628aindexed_dataset\u4e2d\u7684\u6587\u6863\u62bd\u53d6\u6210samples\u7684indices\u3002 \u4ee3\u7801\u89e3\u6790\uff08\u5220\u6389\u4e86\u4e00\u4e9b\u7b80\u5355\u7684\u7684\u4ee3\u7801\uff09\uff1a def _build_index_mappings ( name , data_prefix , documents , sizes , num_samples , seq_length , seed ): \"\"\"Build doc-idx, sample-idx, and shuffle-idx. doc-idx: is an array (ordered) of documents to be used in training. sample-idx: is the start document index and document offset for each training sample. shuffle-idx: maps the sample index into a random index into sample-idx. \"\"\" # Build the indexed mapping if not exist. if torch . distributed . get_rank () == 0 : if ( not os . path . isfile ( doc_idx_filename )) or \\ ( not os . path . isfile ( sample_idx_filename )) or \\ ( not os . path . isfile ( shuffle_idx_filename )): # For the last epoch, decide whether include the entire epoch # in the global shuffle or not. # If we need only one epoch, then separating last epoch does # not mean anything. if num_epochs == 1 : separate_last_epoch = False print ( ' > only one epoch required, setting ' 'separate_last_epoch to False' , flush = True ) else : # Get the number of samples for the last epoch num_samples_from_epochs_minus_one = ( ( num_epochs - 1 ) * tokens_per_epoch - 1 ) // seq_length last_epoch_num_samples = num_samples - \\ num_samples_from_epochs_minus_one assert last_epoch_num_samples >= 0 , \\ 'last epoch number of samples should be non-negative.' num_samples_per_epoch = ( tokens_per_epoch - 1 ) // seq_length assert last_epoch_num_samples < ( num_samples_per_epoch + 1 ), \\ 'last epoch number of samples exceeded max value.' # If we have less than 80% of the samples for the last epoch, # seperate out the epoch and treat it differently. # Note: the 80% number is just based on common sense and can # be adjusted if needed. separate_last_epoch = ( last_epoch_num_samples < int ( 0.80 * num_samples_per_epoch )) if separate_last_epoch : string = ' > last epoch number of samples ( {} ) is smaller ' \\ 'than 80 % o f number of samples per epoch ( {} ), ' \\ 'setting separate_last_epoch to True' else : string = ' > last epoch number of samples ( {} ) is larger ' \\ 'than 80 % o f number of samples per epoch ( {} ), ' \\ 'setting separate_last_epoch to False' print ( string . format ( last_epoch_num_samples , num_samples_per_epoch ), flush = True ) # doc-idx. start_time = time . time () # \u751f\u6210\u6253\u4e71\u540e\u7684doc_idx, shape\u662f(len(documents) * epochs, ) doc_idx = _build_doc_idx ( documents , num_epochs , np_rng , separate_last_epoch ) np . save ( doc_idx_filename , doc_idx , allow_pickle = True ) print_rank_0 ( ' > elasped time to build and save doc-idx mapping ' '(seconds): {:4f} ' . format ( time . time () - start_time )) # sample-idx. start_time = time . time () # Use C++ implementation for speed. # First compile and then import. from megatron.data import helpers assert doc_idx . dtype == np . int32 assert sizes . dtype == np . int32 # \u8fd9\u91cc\u53ef\u4ee5\u89c1\u4e0a\u9762\u7684\u4ee3\u7801\u89e3\u6790\uff0c\u628adoc_idx\u548csizes\u4e00\u8d77\u751f\u6210sample_idx\uff0csample_idx\u662f[doc_idx_index, offset]\u7684list sample_idx = helpers . build_sample_idx ( sizes , doc_idx , seq_length , num_epochs , tokens_per_epoch ) # sample_idx = _build_sample_idx(sizes, doc_idx, seq_length, # num_epochs, tokens_per_epoch) np . save ( sample_idx_filename , sample_idx , allow_pickle = True ) print_rank_0 ( ' > elasped time to build and save sample-idx mapping ' '(seconds): {:4f} ' . format ( time . time () - start_time )) # shuffle-idx. start_time = time . time () # -1 is due to data structure used to retieve the index: # sample i --> [sample_idx[i], sample_idx[i+1]) if separate_last_epoch : num_samples_ = num_samples_from_epochs_minus_one else : num_samples_ = sample_idx . shape [ 0 ] - 1 # \u8fd9\u91cc\u5e94\u8be5\u5c31\u662f\u5355\u7eaf\u7684\u6253\u4e71\uff0c\u6240\u4ee5\u4e0d\u770b\u4ee3\u7801\u5b9e\u73b0\u4e86 shuffle_idx = _build_shuffle_idx ( num_samples_ , sample_idx . shape [ 0 ] - 1 , np_rng ) np . save ( shuffle_idx_filename , shuffle_idx , allow_pickle = True ) print_rank_0 ( ' > elasped time to build and save shuffle-idx mapping' ' (seconds): {:4f} ' . format ( time . time () - start_time )) return doc_idx , sample_idx , shuffle_idx","title":"GPTDataset"},{"location":"GPTDataset/#gptdataset","text":"\u51fd\u6570\u5730\u5740\uff1a https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/gpt_dataset.py#L139-L187 \u51fd\u6570\u529f\u80fd\uff1a \u521b\u5efa\u9002\u7528\u4e8eGPT\u6a21\u578b\u7684dataset\u3002 class GPTDataset ( torch . utils . data . Dataset ): def __init__ ( self , name , data_prefix , documents , indexed_dataset , num_samples , seq_length , seed ): self . name = name self . indexed_dataset = indexed_dataset # Checks assert np . min ( documents ) >= 0 assert np . max ( documents ) < indexed_dataset . sizes . shape [ 0 ] # Build index mappings. # doc_idx\u662findexed_dataset\u6240\u6709document\u590d\u5236num_epochs\u904d\u4e4b\u540e\u6253\u4e71\u7684\u7ed3\u679c\uff0cshape=(len(document) * num_epochs, ) # sample_idx\u662f[doc_idx_index, offset]\u7684list\uff0cshape=(num_samples+1, 2)\uff0c\u6bcf\u4e24\u884c\u4e4b\u95f4\u662f\u4e00\u4e2asample # \u8fd9\u91cc\u591a\u8bf4\u4e00\u53e5\uff0csample_idx\u662f\u4e2a\u533a\u95f4\u7aef\u70b9\u7684idx\uff0c\u6240\u4ee5\u8981\u53d6\u4e00\u4e2asample\u9700\u8981\u7528\u4e24\u4e2a\u5143\u7d20 # \u6bd4\u5982\u53d6\u7b2c3\u4e2asample\uff08\u4ece0\u5f00\u59cb\u8ba1\u6570\uff09\uff0c\u5e94\u8be5\u7528sample_idx[3]\u548csample_idx[4]\u6765\u53d6\uff0c # \u5982\u679csample_idx[3] = [4, 10], sample_idx[4] = [5, 2] # \u90a3\u4e48\u7b2c\u4e09\u4e2asample\u5c31\u662findexed_dataest\u4e2d\u7b2c4\u4e2a\u6587\u6863\u7b2c10\u4e2a\u8bcd\u5f00\u59cb\uff0c\u5230\u7b2c5\u4e2a\u6587\u6863\u7b2c2\u4e2a\u8bcd\u7ed3\u675f\uff08\u4e0d\u5305\u542b\u7b2c2\u4e2a\u8bcd\uff09 # shuffle_idx\u5e94\u8be5\u662fshape=(sample_idx.shape[0], )\uff0c\u5355\u7eaf\u7528\u4e8e\u6253\u4e71 self . doc_idx , self . sample_idx , self . shuffle_idx = _build_index_mappings ( self . name , data_prefix , documents , self . indexed_dataset . sizes , num_samples , seq_length , seed ) def __len__ ( self ): # -1 is due to data structure used to retieve the index: # sample i --> [sample_idx[i], sample_idx[i+1]) return self . sample_idx . shape [ 0 ] - 1 def __getitem__ ( self , idx ): # Get the shuffled index. idx = self . shuffle_idx [ idx ] # Start and end documents and offsets. doc_index_f = self . sample_idx [ idx ][ 0 ] doc_index_l = self . sample_idx [ idx + 1 ][ 0 ] offset_f = self . sample_idx [ idx ][ 1 ] offset_l = self . sample_idx [ idx + 1 ][ 1 ] # If we are within the same document, just extract the chunk. # \u8fd9\u91cc\u5176\u5b9e\u6ca1\u770b\u61c2length\u4e3a\u4ec0\u4e48\u8981+1 if doc_index_f == doc_index_l : sample = self . indexed_dataset . get ( self . doc_idx [ doc_index_f ], offset = offset_f , length = offset_l - offset_f + 1 ) else : # Otherwise, get the rest of the initial document. sample_list = [ self . indexed_dataset . get ( self . doc_idx [ doc_index_f ], offset = offset_f )] # Loop over all in between documents and add the entire document. for i in range ( doc_index_f + 1 , doc_index_l ): sample_list . append ( self . indexed_dataset . get ( self . doc_idx [ i ])) # And finally add the relevant portion of last document. sample_list . append ( self . indexed_dataset . get ( self . doc_idx [ doc_index_l ], length = offset_l + 1 )) sample = np . concatenate ( sample_list ) return { 'text' : np . array ( sample , dtype = np . int64 )}","title":"GPTDataset"},{"location":"GPTDataset/#build_sample_idx","text":"\u51fd\u6570\u5730\u5740\uff1a https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/helpers.cpp#L99-L185 \u51fd\u6570\u529f\u80fd\uff1a \u628aindexed_dataset\u7684\u6240\u6709document\u94fa\u5e73\uff0c\u6309\u7167\u7ed9\u5b9a\u7684seq_length\u5207\u5f00\uff0c\u751f\u6210\u4e0d\u540c\u7684\u8bad\u7ec3sample\uff0c\u8f93\u51fa\u7684sample_idx\u662f\u4e00\u4e2an*2\u7684\u6570\u7ec4\u3002\u6bcf\u4e00\u884c\u7684\u7b2c\u4e00\u5217\u662f\u8be5sample\u5bf9\u5e94\u7684doc_idx\uff0c\u7b2c\u4e8c\u5217\u662f\u8be5sample\u5f00\u5934\u4f4d\u7f6e\u5728\u8be5doc_idx\u4e0bdoc\u5bf9\u5e94\u7684offset\u3002 py :: array build_sample_idx ( const py :: array_t < int32_t > & sizes_ , const py :: array_t < int32_t > & doc_idx_ , const int32_t seq_length , const int32_t num_epochs , const int64_t tokens_per_epoch ) { /* Sample index (sample_idx) is used for gpt2 like dataset for which the documents are flattened and the samples are built based on this 1-D flatten array. It is a 2D array with sizes [number-of-samples + 1, 2] where [..., 0] contains the index into `doc_idx` and [..., 1] is the starting offset in that document.*/ // \u8fd9\u91cc\u8f93\u5165sizes_\u53c2\u6570\u662f\u6bcf\u4e2asentence\u7684\u957f\u5ea6\uff0cdoc_idx_\u662f\u6bcf\u4e2adoc\u5f00\u5934\u7684\u7d22\u5f15 // Consistency checks. assert ( seq_length > 1 ); assert ( num_epochs > 0 ); assert ( tokens_per_epoch > 1 ); // Remove bound checks. auto sizes = sizes_ . unchecked < 1 > (); auto doc_idx = doc_idx_ . unchecked < 1 > (); // Mapping and it's length (1D). int64_t num_samples = ( num_epochs * tokens_per_epoch - 1 ) / seq_length ; int32_t * sample_idx = new int32_t [ 2 * ( num_samples + 1 )]; cout << \" using:\" << endl << std :: flush ; cout << \" number of documents: \" << doc_idx_ . shape ( 0 ) / num_epochs << endl << std :: flush ; cout << \" number of epochs: \" << num_epochs << endl << std :: flush ; cout << \" sequence length: \" << seq_length << endl << std :: flush ; cout << \" total number of samples: \" << num_samples << endl << std :: flush ; // Index into sample_idx. int64_t sample_index = 0 ; // Index into doc_idx. int64_t doc_idx_index = 0 ; // Begining offset for each document. int32_t doc_offset = 0 ; // Start with first document and no offset. sample_idx [ 2 * sample_index ] = doc_idx_index ; sample_idx [ 2 * sample_index + 1 ] = doc_offset ; ++ sample_index ; while ( sample_index <= num_samples ) { // Start with a fresh sequence. int32_t remaining_seq_length = seq_length + 1 ; while ( remaining_seq_length != 0 ) { // Get the document length. auto doc_id = doc_idx [ doc_idx_index ]; auto doc_length = sizes [ doc_id ] - doc_offset ; // And add it to the current sequence. remaining_seq_length -= doc_length ; // If we have more than a full sequence, adjust offset and set // remaining length to zero so we return from the while loop. // Note that -1 here is for the same reason we have -1 in // `_num_epochs` calculations. // \u8fd9\u91cc\u5f53remaining_seq_length <= 0\u7684\u65f6\u5019\uff0cdoc_idx_index\u4e0d\u53d8\uff0c\u4f46\u662foffset\u8981\u589e\u52a0\u5230\u5f53\u524ddoc\u7684\u6700\u540e\u4e00\u4e2atoken\uff08why\uff09 if ( remaining_seq_length <= 0 ) { doc_offset += ( remaining_seq_length + doc_length - 1 ); remaining_seq_length = 0 ; } else { // Otherwise, start from the begining of the next document. ++ doc_idx_index ; doc_offset = 0 ; } } // Record the sequence. sample_idx [ 2 * sample_index ] = doc_idx_index ; sample_idx [ 2 * sample_index + 1 ] = doc_offset ; ++ sample_index ; } // Method to deallocate memory. py :: capsule free_when_done ( sample_idx , []( void * mem_ ) { int32_t * mem = reinterpret_cast < int32_t *> ( mem_ ); delete [] mem ; }); // Return the numpy array. const auto byte_size = sizeof ( int32_t ); return py :: array ( std :: vector < int64_t > { num_samples + 1 , 2 }, // shape { 2 * byte_size , byte_size }, // C-style contiguous strides sample_idx , // the data pointer free_when_done ); // numpy array references }","title":"build_sample_idx"},{"location":"GPTDataset/#_build_doc_idx","text":"\u51fd\u6570\u5730\u5740\uff1a https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/gpt_dataset.py#L346-L359 \u51fd\u6570\u529f\u80fd\uff1a \u628adocuments. (np.ndarray) \u590d\u5236num_epochs\u4efd\uff0c\u7136\u540eshuffle\uff0c\u867d\u7136\u6211\u4e0d\u77e5\u9053\u4e3a\u4ec0\u4e48\u8fd9\u4e48\u505a\u3002\u6253\u4e71\u4e4b\u540e\uff0c\u540c\u4e00\u4e2adoc\u53ef\u80fd\u5728\u4e00\u4e2aepoch\u8bad\u7ec3\u591a\u6b21\u3002 def _build_doc_idx ( documents , num_epochs , np_rng , separate_last_epoch ): \"\"\"Build an array with length = number-of-epochs * number-of-dcuments. Each index is mapped to a corresponding document.\"\"\" if not separate_last_epoch or num_epochs == 1 : # doc_idx\u662f\u4e00\u4e2a2\u7ef4np.ndarray\uff0c\u4e00\u5171num_epochs\u884c\uff0c\u6bcf\u4e00\u884c\u662fnp.arange(len(documents)) doc_idx = np . mgrid [ 0 : num_epochs , 0 : len ( documents )][ 1 ] doc_idx [:] = documents # flatten\u4e4b\u540e\u518d\u6253\u4e71\uff0c\u6bd4\u5982[[1, 2, 3], [1, 2, 3]]\u53d8\u6210[2, 3, 2, 1, 3, 1] doc_idx = doc_idx . reshape ( - 1 ) doc_idx = doc_idx . astype ( np . int32 ) np_rng . shuffle ( doc_idx ) return doc_idx doc_idx_first = _build_doc_idx ( documents , num_epochs - 1 , np_rng , False ) doc_idx_last = _build_doc_idx ( documents , 1 , np_rng , False ) return np . concatenate (( doc_idx_first , doc_idx_last ))","title":"_build_doc_idx"},{"location":"GPTDataset/#_build_index_mappings","text":"\u51fd\u6570\u5730\u5740\uff1a https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/gpt_dataset.py#L190-L323 \u51fd\u6570\u529f\u80fd\uff1a \u628aindexed_dataset\u4e2d\u7684\u6587\u6863\u62bd\u53d6\u6210samples\u7684indices\u3002 \u4ee3\u7801\u89e3\u6790\uff08\u5220\u6389\u4e86\u4e00\u4e9b\u7b80\u5355\u7684\u7684\u4ee3\u7801\uff09\uff1a def _build_index_mappings ( name , data_prefix , documents , sizes , num_samples , seq_length , seed ): \"\"\"Build doc-idx, sample-idx, and shuffle-idx. doc-idx: is an array (ordered) of documents to be used in training. sample-idx: is the start document index and document offset for each training sample. shuffle-idx: maps the sample index into a random index into sample-idx. \"\"\" # Build the indexed mapping if not exist. if torch . distributed . get_rank () == 0 : if ( not os . path . isfile ( doc_idx_filename )) or \\ ( not os . path . isfile ( sample_idx_filename )) or \\ ( not os . path . isfile ( shuffle_idx_filename )): # For the last epoch, decide whether include the entire epoch # in the global shuffle or not. # If we need only one epoch, then separating last epoch does # not mean anything. if num_epochs == 1 : separate_last_epoch = False print ( ' > only one epoch required, setting ' 'separate_last_epoch to False' , flush = True ) else : # Get the number of samples for the last epoch num_samples_from_epochs_minus_one = ( ( num_epochs - 1 ) * tokens_per_epoch - 1 ) // seq_length last_epoch_num_samples = num_samples - \\ num_samples_from_epochs_minus_one assert last_epoch_num_samples >= 0 , \\ 'last epoch number of samples should be non-negative.' num_samples_per_epoch = ( tokens_per_epoch - 1 ) // seq_length assert last_epoch_num_samples < ( num_samples_per_epoch + 1 ), \\ 'last epoch number of samples exceeded max value.' # If we have less than 80% of the samples for the last epoch, # seperate out the epoch and treat it differently. # Note: the 80% number is just based on common sense and can # be adjusted if needed. separate_last_epoch = ( last_epoch_num_samples < int ( 0.80 * num_samples_per_epoch )) if separate_last_epoch : string = ' > last epoch number of samples ( {} ) is smaller ' \\ 'than 80 % o f number of samples per epoch ( {} ), ' \\ 'setting separate_last_epoch to True' else : string = ' > last epoch number of samples ( {} ) is larger ' \\ 'than 80 % o f number of samples per epoch ( {} ), ' \\ 'setting separate_last_epoch to False' print ( string . format ( last_epoch_num_samples , num_samples_per_epoch ), flush = True ) # doc-idx. start_time = time . time () # \u751f\u6210\u6253\u4e71\u540e\u7684doc_idx, shape\u662f(len(documents) * epochs, ) doc_idx = _build_doc_idx ( documents , num_epochs , np_rng , separate_last_epoch ) np . save ( doc_idx_filename , doc_idx , allow_pickle = True ) print_rank_0 ( ' > elasped time to build and save doc-idx mapping ' '(seconds): {:4f} ' . format ( time . time () - start_time )) # sample-idx. start_time = time . time () # Use C++ implementation for speed. # First compile and then import. from megatron.data import helpers assert doc_idx . dtype == np . int32 assert sizes . dtype == np . int32 # \u8fd9\u91cc\u53ef\u4ee5\u89c1\u4e0a\u9762\u7684\u4ee3\u7801\u89e3\u6790\uff0c\u628adoc_idx\u548csizes\u4e00\u8d77\u751f\u6210sample_idx\uff0csample_idx\u662f[doc_idx_index, offset]\u7684list sample_idx = helpers . build_sample_idx ( sizes , doc_idx , seq_length , num_epochs , tokens_per_epoch ) # sample_idx = _build_sample_idx(sizes, doc_idx, seq_length, # num_epochs, tokens_per_epoch) np . save ( sample_idx_filename , sample_idx , allow_pickle = True ) print_rank_0 ( ' > elasped time to build and save sample-idx mapping ' '(seconds): {:4f} ' . format ( time . time () - start_time )) # shuffle-idx. start_time = time . time () # -1 is due to data structure used to retieve the index: # sample i --> [sample_idx[i], sample_idx[i+1]) if separate_last_epoch : num_samples_ = num_samples_from_epochs_minus_one else : num_samples_ = sample_idx . shape [ 0 ] - 1 # \u8fd9\u91cc\u5e94\u8be5\u5c31\u662f\u5355\u7eaf\u7684\u6253\u4e71\uff0c\u6240\u4ee5\u4e0d\u770b\u4ee3\u7801\u5b9e\u73b0\u4e86 shuffle_idx = _build_shuffle_idx ( num_samples_ , sample_idx . shape [ 0 ] - 1 , np_rng ) np . save ( shuffle_idx_filename , shuffle_idx , allow_pickle = True ) print_rank_0 ( ' > elasped time to build and save shuffle-idx mapping' ' (seconds): {:4f} ' . format ( time . time () - start_time )) return doc_idx , sample_idx , shuffle_idx","title":"_build_index_mappings"},{"location":"create_masked_lm_predictions/","text":"Megatron \u6e90\u7801\u9605\u8bfb\u7b14\u8bb0 \u2014\u2014 create_masked_lm_predictions \u4ee3\u7801\u5730\u5740\u5728 https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/dataset_utils.py#L181-L380 def create_masked_lm_predictions ( tokens , vocab_id_list , vocab_id_to_token_dict , masked_lm_prob , cls_id , sep_id , mask_id , max_predictions_per_seq , np_rng , max_ngrams = 3 , do_whole_word_mask = True , favor_longer_ngram = False , do_permutation = False , geometric_dist = False , masking_style = \"bert\" ): \"\"\"Creates the predictions for the masked LM objective. Note: Tokens here are vocab ids and not text tokens.\"\"\" cand_indexes = [] # Note(mingdachen): We create a list for recording if the piece is # the starting piece of current token, where 1 means true, so that # on-the-fly whole word masking is possible. # token_boundary\u662f\u4e00\u4e2a\u957f\u5ea6\u548ctokens\u76f8\u540c\u7684list\uff0c\u4fdd\u5b58\u7684\u503c\u53ea\u67091\u548c0 # 1\u8868\u793a\u5bf9\u5e94\u4f4d\u7f6e\u7684token\u662f\u4e00\u4e2a\u8bcd\u8bed\u7684\u5f00\u5934piece\uff0c0\u8868\u793a\u5bf9\u5e94\u4f4d\u7f6e\u7684token\u4e0d\u662f\u4e00\u4e2a\u8bcd\u8bed\u7684\u5f00\u5934\uff0c\u6bd4\u5982 ##ing\u8fd9\u6837 # \u4e4b\u6240\u4ee5\u8981\u8fd9\u4e48\u8bb0\u5f55\uff0c\u662f\u56e0\u4e3awhole word mask\u9700\u8981\u628a\u6574\u4e2aword\u8fdb\u884cmask # \u5982\u679c\u968f\u673a\u9009\u5230\u4e86\u67d0\u4e2aword piece\u8fdb\u884cmask\uff0c\u90a3\u4e48\u548c\u5b83\u5c5e\u4e8e\u540c\u4e00\u4e2a\u8bcd\u7684\u5176\u4ed6piece\u90fd\u8981mask token_boundary = [ 0 ] * len ( tokens ) # \u4e0b\u9762\u8fd9\u4e2a\u5faa\u73af\u7528\u6765\u4fee\u6539token_boundary\u8fd9\u4e2alist\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u503c for ( i , token ) in enumerate ( tokens ): if token == cls_id or token == sep_id : token_boundary [ i ] = 1 continue # Whole Word Masking means that if we mask all of the wordpieces # corresponding to an original word. # # Note that Whole Word Masking does *not* change the training code # at all -- we still predict each WordPiece independently, softmaxed # over the entire vocabulary. # is_start_piece\u7528\u6765\u5224\u65ad\u662f\u5426\u662f\u5f00\u5934\u7684word piece\uff0c\u5224\u65ad\u65b9\u6cd5\u5c31\u770btoken\u5bf9\u5e94\u7684word piece\u662f\u5426\u4ee5##\u5f00\u5934 if ( do_whole_word_mask and len ( cand_indexes ) >= 1 and not is_start_piece ( vocab_id_to_token_dict [ token ])): cand_indexes [ - 1 ] . append ( i ) else : cand_indexes . append ([ i ]) if is_start_piece ( vocab_id_to_token_dict [ token ]): token_boundary [ i ] = 1 # output_tokens\u662f\u8f93\u5165tokens\u7684\u4e00\u4e2a\u526f\u672c\uff0c\u7528\u6765\u8f93\u51famask\u4e4b\u540e\u7684token output_tokens = list ( tokens ) # \u4fdd\u5b58mask\u6389\u6570\u636e\u7684\u4f4d\u7f6e\u548c\u5bf9\u5e94\u7684label\uff08label\u5176\u5b9e\u5c31\u662ftoken_id\uff09 masked_lm_positions = [] masked_lm_labels = [] if masked_lm_prob == 0 : return ( output_tokens , masked_lm_positions , masked_lm_labels , token_boundary ) # \u8fd9\u91cc\u8ba1\u7b97\u8981mask\u7684\u6570\u91cf\uff0c\u6709\u4e00\u4e2amax_predictions_per_seq\u7528\u6765\u7ea6\u675f\u6700\u5927\u503c num_to_predict = min ( max_predictions_per_seq , max ( 1 , int ( round ( len ( tokens ) * masked_lm_prob )))) # ngrams\u4fdd\u5b58\u88abmask\u7684word\u5e8f\u5217\u6700\u5927\u957f\u5ea6\uff08\u4e0d\u662fword piece\uff09 # \u6bd4\u5982ngrams==3\uff0c\u90a3\u4e48\u6700\u591a\u53ef\u4ee5\u8fde\u7eedmask\u4e09\u4e2a\u5355\u8bcd ngrams = np . arange ( 1 , max_ngrams + 1 , dtype = np . int64 ) # \u8fd9\u91ccpval\u662f\u4e0d\u540c\u957f\u5ea6\u5355\u8bcd\u5e8f\u5217\u88abmask\u7684\u6982\u7387\uff0c\u6bd4\u5982ngrams==3\uff0c\u90a3\u4e48\u6982\u7387\u5c31\u662f(1, 1/2, 1/3)\uff0c\u5f52\u4e00\u5316\u5c31\u662f(6/11, 3/11, 2/11) if not geometric_dist : # Note(mingdachen): # By default, we set the probilities to favor shorter ngram sequences. pvals = 1. / np . arange ( 1 , max_ngrams + 1 ) pvals /= pvals . sum ( keepdims = True ) if favor_longer_ngram : pvals = pvals [:: - 1 ] # \u8fd9\u91ccngram_indexes\u4fdd\u5b58\u6240\u6709\u957f\u5ea6==[1, max_ngrams)\u7684\u5355\u8bcd\u5e8f\u5217 # \u6bd4\u5982tokens=[[1], [2, 3], [4]]\uff0cmax_ngrams==3\uff0c # \u90a3\u4e48ngram_indexes==[[[1]], [[1], [2, 3]], [[1], [2, 3], [4]], [[2, 3]], [[2, 3], [4]], [[4]]] ngram_indexes = [] for idx in range ( len ( cand_indexes )): ngram_index = [] for n in ngrams : ngram_index . append ( cand_indexes [ idx : idx + n ]) ngram_indexes . append ( ngram_index ) # \u8fd9\u91ccshuffle\u4e3a\u4e86\u4e4b\u540e\u968f\u673a\u91c7\u6837\u505a\u51c6\u5907\uff0c\u5148shuffle\uff0c\u518d\u62ff\u524dn\u4e2a\uff0c\u5c31\u76f8\u5f53\u4e8e\u968f\u673a\u91c7\u6837n\u4e2a np_rng . shuffle ( ngram_indexes ) ( masked_lms , masked_spans ) = ([], []) # covered_indexes\u4fdd\u5b58\u88abmask\u8fc7\u7684indexes covered_indexes = set () for cand_index_set in ngram_indexes : # \u5982\u679cmask\u6389\u7684\u5355\u8bcd\u6570\u91cf\u8d85\u8fc7\u9884\u671f\u5c31\u7ed3\u675fmask\u8fc7\u7a0b if len ( masked_lms ) >= num_to_predict : break if not cand_index_set : continue # Note(mingdachen): # Skip current piece if they are covered in lm masking or previous ngrams. # \u8fd9\u4e2a\u5faa\u73af\u786e\u5b9e\u6ca1\u770b\u61c2\u5728\u5e72\u5565 for index_set in cand_index_set [ 0 ]: for index in index_set : if index in covered_indexes : continue # \u6839\u636e\u4e4b\u524d\u7684pval\u9009\u62e9\u4e00\u4e2angrams\u7684\u503c\u6765\u9009\u62e9mask\u7684\u5355\u8bcd\u5e8f\u5217\u7684\u957f\u5ea6 if not geometric_dist : n = np_rng . choice ( ngrams [: len ( cand_index_set )], p = pvals [: len ( cand_index_set )] / pvals [: len ( cand_index_set )] . sum ( keepdims = True )) else : # Sampling \"n\" from the geometric distribution and clipping it to # the max_ngrams. Using p=0.2 default from the SpanBERT paper # https://arxiv.org/pdf/1907.10529.pdf (Sec 3.1) n = min ( np_rng . geometric ( 0.2 ), max_ngrams ) # \u628acand_index_set[0: n]\u7684\u6240\u6709\u5143\u7d20concat\u8d77\u6765 index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 # Note(mingdachen): # Repeatedly looking for a candidate that does not exceed the # maximum number of predictions by trying shorter ngrams. while len ( masked_lms ) + len ( index_set ) > num_to_predict : if n == 0 : break index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 # If adding a whole-word mask would exceed the maximum number of # predictions, then just skip this candidate. if len ( masked_lms ) + len ( index_set ) > num_to_predict : continue is_any_index_covered = False for index in index_set : if index in covered_indexes : is_any_index_covered = True break if is_any_index_covered : continue for index in index_set : covered_indexes . add ( index ) masked_token = None if masking_style == \"bert\" : # 80% of the time, replace with [MASK] if np_rng . random () < 0.8 : masked_token = mask_id else : # 10% of the time, keep original if np_rng . random () < 0.5 : masked_token = tokens [ index ] # 10% of the time, replace with random word else : masked_token = vocab_id_list [ np_rng . randint ( 0 , len ( vocab_id_list ))] elif masking_style == \"t5\" : masked_token = mask_id else : raise ValueError ( \"invalid value of masking style\" ) output_tokens [ index ] = masked_token masked_lms . append ( MaskedLmInstance ( index = index , label = tokens [ index ])) masked_spans . append ( MaskedLmInstance ( index = index_set , label = [ tokens [ index ] for index in index_set ])) assert len ( masked_lms ) <= num_to_predict np_rng . shuffle ( ngram_indexes ) select_indexes = set () if do_permutation : for cand_index_set in ngram_indexes : if len ( select_indexes ) >= num_to_predict : break if not cand_index_set : continue # Note(mingdachen): # Skip current piece if they are covered in lm masking or previous ngrams. for index_set in cand_index_set [ 0 ]: for index in index_set : if index in covered_indexes or index in select_indexes : continue n = np . random . choice ( ngrams [: len ( cand_index_set )], p = pvals [: len ( cand_index_set )] / pvals [: len ( cand_index_set )] . sum ( keepdims = True )) index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 while len ( select_indexes ) + len ( index_set ) > num_to_predict : if n == 0 : break index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 # If adding a whole-word mask would exceed the maximum number of # predictions, then just skip this candidate. if len ( select_indexes ) + len ( index_set ) > num_to_predict : continue is_any_index_covered = False for index in index_set : if index in covered_indexes or index in select_indexes : is_any_index_covered = True break if is_any_index_covered : continue for index in index_set : select_indexes . add ( index ) assert len ( select_indexes ) <= num_to_predict select_indexes = sorted ( select_indexes ) permute_indexes = list ( select_indexes ) np_rng . shuffle ( permute_indexes ) orig_token = list ( output_tokens ) for src_i , tgt_i in zip ( select_indexes , permute_indexes ): output_tokens [ src_i ] = orig_token [ tgt_i ] masked_lms . append ( MaskedLmInstance ( index = src_i , label = orig_token [ src_i ])) masked_lms = sorted ( masked_lms , key = lambda x : x . index ) # Sort the spans by the index of the first span masked_spans = sorted ( masked_spans , key = lambda x : x . index [ 0 ]) for p in masked_lms : masked_lm_positions . append ( p . index ) masked_lm_labels . append ( p . label ) return ( output_tokens , masked_lm_positions , masked_lm_labels , token_boundary , masked_spans )","title":"Create masked lm predictions"},{"location":"create_masked_lm_predictions/#megatron-create_masked_lm_predictions","text":"\u4ee3\u7801\u5730\u5740\u5728 https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/dataset_utils.py#L181-L380 def create_masked_lm_predictions ( tokens , vocab_id_list , vocab_id_to_token_dict , masked_lm_prob , cls_id , sep_id , mask_id , max_predictions_per_seq , np_rng , max_ngrams = 3 , do_whole_word_mask = True , favor_longer_ngram = False , do_permutation = False , geometric_dist = False , masking_style = \"bert\" ): \"\"\"Creates the predictions for the masked LM objective. Note: Tokens here are vocab ids and not text tokens.\"\"\" cand_indexes = [] # Note(mingdachen): We create a list for recording if the piece is # the starting piece of current token, where 1 means true, so that # on-the-fly whole word masking is possible. # token_boundary\u662f\u4e00\u4e2a\u957f\u5ea6\u548ctokens\u76f8\u540c\u7684list\uff0c\u4fdd\u5b58\u7684\u503c\u53ea\u67091\u548c0 # 1\u8868\u793a\u5bf9\u5e94\u4f4d\u7f6e\u7684token\u662f\u4e00\u4e2a\u8bcd\u8bed\u7684\u5f00\u5934piece\uff0c0\u8868\u793a\u5bf9\u5e94\u4f4d\u7f6e\u7684token\u4e0d\u662f\u4e00\u4e2a\u8bcd\u8bed\u7684\u5f00\u5934\uff0c\u6bd4\u5982 ##ing\u8fd9\u6837 # \u4e4b\u6240\u4ee5\u8981\u8fd9\u4e48\u8bb0\u5f55\uff0c\u662f\u56e0\u4e3awhole word mask\u9700\u8981\u628a\u6574\u4e2aword\u8fdb\u884cmask # \u5982\u679c\u968f\u673a\u9009\u5230\u4e86\u67d0\u4e2aword piece\u8fdb\u884cmask\uff0c\u90a3\u4e48\u548c\u5b83\u5c5e\u4e8e\u540c\u4e00\u4e2a\u8bcd\u7684\u5176\u4ed6piece\u90fd\u8981mask token_boundary = [ 0 ] * len ( tokens ) # \u4e0b\u9762\u8fd9\u4e2a\u5faa\u73af\u7528\u6765\u4fee\u6539token_boundary\u8fd9\u4e2alist\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u503c for ( i , token ) in enumerate ( tokens ): if token == cls_id or token == sep_id : token_boundary [ i ] = 1 continue # Whole Word Masking means that if we mask all of the wordpieces # corresponding to an original word. # # Note that Whole Word Masking does *not* change the training code # at all -- we still predict each WordPiece independently, softmaxed # over the entire vocabulary. # is_start_piece\u7528\u6765\u5224\u65ad\u662f\u5426\u662f\u5f00\u5934\u7684word piece\uff0c\u5224\u65ad\u65b9\u6cd5\u5c31\u770btoken\u5bf9\u5e94\u7684word piece\u662f\u5426\u4ee5##\u5f00\u5934 if ( do_whole_word_mask and len ( cand_indexes ) >= 1 and not is_start_piece ( vocab_id_to_token_dict [ token ])): cand_indexes [ - 1 ] . append ( i ) else : cand_indexes . append ([ i ]) if is_start_piece ( vocab_id_to_token_dict [ token ]): token_boundary [ i ] = 1 # output_tokens\u662f\u8f93\u5165tokens\u7684\u4e00\u4e2a\u526f\u672c\uff0c\u7528\u6765\u8f93\u51famask\u4e4b\u540e\u7684token output_tokens = list ( tokens ) # \u4fdd\u5b58mask\u6389\u6570\u636e\u7684\u4f4d\u7f6e\u548c\u5bf9\u5e94\u7684label\uff08label\u5176\u5b9e\u5c31\u662ftoken_id\uff09 masked_lm_positions = [] masked_lm_labels = [] if masked_lm_prob == 0 : return ( output_tokens , masked_lm_positions , masked_lm_labels , token_boundary ) # \u8fd9\u91cc\u8ba1\u7b97\u8981mask\u7684\u6570\u91cf\uff0c\u6709\u4e00\u4e2amax_predictions_per_seq\u7528\u6765\u7ea6\u675f\u6700\u5927\u503c num_to_predict = min ( max_predictions_per_seq , max ( 1 , int ( round ( len ( tokens ) * masked_lm_prob )))) # ngrams\u4fdd\u5b58\u88abmask\u7684word\u5e8f\u5217\u6700\u5927\u957f\u5ea6\uff08\u4e0d\u662fword piece\uff09 # \u6bd4\u5982ngrams==3\uff0c\u90a3\u4e48\u6700\u591a\u53ef\u4ee5\u8fde\u7eedmask\u4e09\u4e2a\u5355\u8bcd ngrams = np . arange ( 1 , max_ngrams + 1 , dtype = np . int64 ) # \u8fd9\u91ccpval\u662f\u4e0d\u540c\u957f\u5ea6\u5355\u8bcd\u5e8f\u5217\u88abmask\u7684\u6982\u7387\uff0c\u6bd4\u5982ngrams==3\uff0c\u90a3\u4e48\u6982\u7387\u5c31\u662f(1, 1/2, 1/3)\uff0c\u5f52\u4e00\u5316\u5c31\u662f(6/11, 3/11, 2/11) if not geometric_dist : # Note(mingdachen): # By default, we set the probilities to favor shorter ngram sequences. pvals = 1. / np . arange ( 1 , max_ngrams + 1 ) pvals /= pvals . sum ( keepdims = True ) if favor_longer_ngram : pvals = pvals [:: - 1 ] # \u8fd9\u91ccngram_indexes\u4fdd\u5b58\u6240\u6709\u957f\u5ea6==[1, max_ngrams)\u7684\u5355\u8bcd\u5e8f\u5217 # \u6bd4\u5982tokens=[[1], [2, 3], [4]]\uff0cmax_ngrams==3\uff0c # \u90a3\u4e48ngram_indexes==[[[1]], [[1], [2, 3]], [[1], [2, 3], [4]], [[2, 3]], [[2, 3], [4]], [[4]]] ngram_indexes = [] for idx in range ( len ( cand_indexes )): ngram_index = [] for n in ngrams : ngram_index . append ( cand_indexes [ idx : idx + n ]) ngram_indexes . append ( ngram_index ) # \u8fd9\u91ccshuffle\u4e3a\u4e86\u4e4b\u540e\u968f\u673a\u91c7\u6837\u505a\u51c6\u5907\uff0c\u5148shuffle\uff0c\u518d\u62ff\u524dn\u4e2a\uff0c\u5c31\u76f8\u5f53\u4e8e\u968f\u673a\u91c7\u6837n\u4e2a np_rng . shuffle ( ngram_indexes ) ( masked_lms , masked_spans ) = ([], []) # covered_indexes\u4fdd\u5b58\u88abmask\u8fc7\u7684indexes covered_indexes = set () for cand_index_set in ngram_indexes : # \u5982\u679cmask\u6389\u7684\u5355\u8bcd\u6570\u91cf\u8d85\u8fc7\u9884\u671f\u5c31\u7ed3\u675fmask\u8fc7\u7a0b if len ( masked_lms ) >= num_to_predict : break if not cand_index_set : continue # Note(mingdachen): # Skip current piece if they are covered in lm masking or previous ngrams. # \u8fd9\u4e2a\u5faa\u73af\u786e\u5b9e\u6ca1\u770b\u61c2\u5728\u5e72\u5565 for index_set in cand_index_set [ 0 ]: for index in index_set : if index in covered_indexes : continue # \u6839\u636e\u4e4b\u524d\u7684pval\u9009\u62e9\u4e00\u4e2angrams\u7684\u503c\u6765\u9009\u62e9mask\u7684\u5355\u8bcd\u5e8f\u5217\u7684\u957f\u5ea6 if not geometric_dist : n = np_rng . choice ( ngrams [: len ( cand_index_set )], p = pvals [: len ( cand_index_set )] / pvals [: len ( cand_index_set )] . sum ( keepdims = True )) else : # Sampling \"n\" from the geometric distribution and clipping it to # the max_ngrams. Using p=0.2 default from the SpanBERT paper # https://arxiv.org/pdf/1907.10529.pdf (Sec 3.1) n = min ( np_rng . geometric ( 0.2 ), max_ngrams ) # \u628acand_index_set[0: n]\u7684\u6240\u6709\u5143\u7d20concat\u8d77\u6765 index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 # Note(mingdachen): # Repeatedly looking for a candidate that does not exceed the # maximum number of predictions by trying shorter ngrams. while len ( masked_lms ) + len ( index_set ) > num_to_predict : if n == 0 : break index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 # If adding a whole-word mask would exceed the maximum number of # predictions, then just skip this candidate. if len ( masked_lms ) + len ( index_set ) > num_to_predict : continue is_any_index_covered = False for index in index_set : if index in covered_indexes : is_any_index_covered = True break if is_any_index_covered : continue for index in index_set : covered_indexes . add ( index ) masked_token = None if masking_style == \"bert\" : # 80% of the time, replace with [MASK] if np_rng . random () < 0.8 : masked_token = mask_id else : # 10% of the time, keep original if np_rng . random () < 0.5 : masked_token = tokens [ index ] # 10% of the time, replace with random word else : masked_token = vocab_id_list [ np_rng . randint ( 0 , len ( vocab_id_list ))] elif masking_style == \"t5\" : masked_token = mask_id else : raise ValueError ( \"invalid value of masking style\" ) output_tokens [ index ] = masked_token masked_lms . append ( MaskedLmInstance ( index = index , label = tokens [ index ])) masked_spans . append ( MaskedLmInstance ( index = index_set , label = [ tokens [ index ] for index in index_set ])) assert len ( masked_lms ) <= num_to_predict np_rng . shuffle ( ngram_indexes ) select_indexes = set () if do_permutation : for cand_index_set in ngram_indexes : if len ( select_indexes ) >= num_to_predict : break if not cand_index_set : continue # Note(mingdachen): # Skip current piece if they are covered in lm masking or previous ngrams. for index_set in cand_index_set [ 0 ]: for index in index_set : if index in covered_indexes or index in select_indexes : continue n = np . random . choice ( ngrams [: len ( cand_index_set )], p = pvals [: len ( cand_index_set )] / pvals [: len ( cand_index_set )] . sum ( keepdims = True )) index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 while len ( select_indexes ) + len ( index_set ) > num_to_predict : if n == 0 : break index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 # If adding a whole-word mask would exceed the maximum number of # predictions, then just skip this candidate. if len ( select_indexes ) + len ( index_set ) > num_to_predict : continue is_any_index_covered = False for index in index_set : if index in covered_indexes or index in select_indexes : is_any_index_covered = True break if is_any_index_covered : continue for index in index_set : select_indexes . add ( index ) assert len ( select_indexes ) <= num_to_predict select_indexes = sorted ( select_indexes ) permute_indexes = list ( select_indexes ) np_rng . shuffle ( permute_indexes ) orig_token = list ( output_tokens ) for src_i , tgt_i in zip ( select_indexes , permute_indexes ): output_tokens [ src_i ] = orig_token [ tgt_i ] masked_lms . append ( MaskedLmInstance ( index = src_i , label = orig_token [ src_i ])) masked_lms = sorted ( masked_lms , key = lambda x : x . index ) # Sort the spans by the index of the first span masked_spans = sorted ( masked_spans , key = lambda x : x . index [ 0 ]) for p in masked_lms : masked_lm_positions . append ( p . index ) masked_lm_labels . append ( p . label ) return ( output_tokens , masked_lm_positions , masked_lm_labels , token_boundary , masked_spans )","title":"Megatron \u6e90\u7801\u9605\u8bfb\u7b14\u8bb0 \u2014\u2014 create_masked_lm_predictions"},{"location":"note/","text":"Megatron \u6e90\u7801\u9605\u8bfb\u7b14\u8bb0 \u2014\u2014 create_masked_lm_predictions \u4ee3\u7801\u5730\u5740\u5728 https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/dataset_utils.py#L181-L380 def create_masked_lm_predictions ( tokens , vocab_id_list , vocab_id_to_token_dict , masked_lm_prob , cls_id , sep_id , mask_id , max_predictions_per_seq , np_rng , max_ngrams = 3 , do_whole_word_mask = True , favor_longer_ngram = False , do_permutation = False , geometric_dist = False , masking_style = \"bert\" ): \"\"\"Creates the predictions for the masked LM objective. Note: Tokens here are vocab ids and not text tokens.\"\"\" cand_indexes = [] # Note(mingdachen): We create a list for recording if the piece is # the starting piece of current token, where 1 means true, so that # on-the-fly whole word masking is possible. # token_boundary\u662f\u4e00\u4e2a\u957f\u5ea6\u548ctokens\u76f8\u540c\u7684list\uff0c\u4fdd\u5b58\u7684\u503c\u53ea\u67091\u548c0 # 1\u8868\u793a\u5bf9\u5e94\u4f4d\u7f6e\u7684token\u662f\u4e00\u4e2a\u8bcd\u8bed\u7684\u5f00\u5934piece\uff0c0\u8868\u793a\u5bf9\u5e94\u4f4d\u7f6e\u7684token\u4e0d\u662f\u4e00\u4e2a\u8bcd\u8bed\u7684\u5f00\u5934\uff0c\u6bd4\u5982 ##ing\u8fd9\u6837 # \u4e4b\u6240\u4ee5\u8981\u8fd9\u4e48\u8bb0\u5f55\uff0c\u662f\u56e0\u4e3awhole word mask\u9700\u8981\u628a\u6574\u4e2aword\u8fdb\u884cmask # \u5982\u679c\u968f\u673a\u9009\u5230\u4e86\u67d0\u4e2aword piece\u8fdb\u884cmask\uff0c\u90a3\u4e48\u548c\u5b83\u5c5e\u4e8e\u540c\u4e00\u4e2a\u8bcd\u7684\u5176\u4ed6piece\u90fd\u8981mask token_boundary = [ 0 ] * len ( tokens ) # \u4e0b\u9762\u8fd9\u4e2a\u5faa\u73af\u7528\u6765\u4fee\u6539token_boundary\u8fd9\u4e2alist\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u503c for ( i , token ) in enumerate ( tokens ): if token == cls_id or token == sep_id : token_boundary [ i ] = 1 continue # Whole Word Masking means that if we mask all of the wordpieces # corresponding to an original word. # # Note that Whole Word Masking does *not* change the training code # at all -- we still predict each WordPiece independently, softmaxed # over the entire vocabulary. # is_start_piece\u7528\u6765\u5224\u65ad\u662f\u5426\u662f\u5f00\u5934\u7684word piece\uff0c\u5224\u65ad\u65b9\u6cd5\u5c31\u770btoken\u5bf9\u5e94\u7684word piece\u662f\u5426\u4ee5##\u5f00\u5934 if ( do_whole_word_mask and len ( cand_indexes ) >= 1 and not is_start_piece ( vocab_id_to_token_dict [ token ])): cand_indexes [ - 1 ] . append ( i ) else : cand_indexes . append ([ i ]) if is_start_piece ( vocab_id_to_token_dict [ token ]): token_boundary [ i ] = 1 # output_tokens\u662f\u8f93\u5165tokens\u7684\u4e00\u4e2a\u526f\u672c\uff0c\u7528\u6765\u8f93\u51famask\u4e4b\u540e\u7684token output_tokens = list ( tokens ) # \u4fdd\u5b58mask\u6389\u6570\u636e\u7684\u4f4d\u7f6e\u548c\u5bf9\u5e94\u7684label\uff08label\u5176\u5b9e\u5c31\u662ftoken_id\uff09 masked_lm_positions = [] masked_lm_labels = [] if masked_lm_prob == 0 : return ( output_tokens , masked_lm_positions , masked_lm_labels , token_boundary ) # \u8fd9\u91cc\u8ba1\u7b97\u8981mask\u7684\u6570\u91cf\uff0c\u6709\u4e00\u4e2amax_predictions_per_seq\u7528\u6765\u7ea6\u675f\u6700\u5927\u503c num_to_predict = min ( max_predictions_per_seq , max ( 1 , int ( round ( len ( tokens ) * masked_lm_prob )))) # ngrams\u4fdd\u5b58\u88abmask\u7684word\u5e8f\u5217\u6700\u5927\u957f\u5ea6\uff08\u4e0d\u662fword piece\uff09 # \u6bd4\u5982ngrams==3\uff0c\u90a3\u4e48\u6700\u591a\u53ef\u4ee5\u8fde\u7eedmask\u4e09\u4e2a\u5355\u8bcd ngrams = np . arange ( 1 , max_ngrams + 1 , dtype = np . int64 ) # \u8fd9\u91ccpval\u662f\u4e0d\u540c\u957f\u5ea6\u5355\u8bcd\u5e8f\u5217\u88abmask\u7684\u6982\u7387\uff0c\u6bd4\u5982ngrams==3\uff0c\u90a3\u4e48\u6982\u7387\u5c31\u662f(1, 1/2, 1/3)\uff0c\u5f52\u4e00\u5316\u5c31\u662f(6/11, 3/11, 2/11) if not geometric_dist : # Note(mingdachen): # By default, we set the probilities to favor shorter ngram sequences. pvals = 1. / np . arange ( 1 , max_ngrams + 1 ) pvals /= pvals . sum ( keepdims = True ) if favor_longer_ngram : pvals = pvals [:: - 1 ] # \u8fd9\u91ccngram_indexes\u4fdd\u5b58\u6240\u6709\u957f\u5ea6==[1, max_ngrams)\u7684\u5355\u8bcd\u5e8f\u5217 # \u6bd4\u5982tokens=[[1], [2, 3], [4]]\uff0cmax_ngrams==3\uff0c # \u90a3\u4e48ngram_indexes==[[[1]], [[1], [2, 3]], [[1], [2, 3], [4]], [[2, 3]], [[2, 3], [4]], [[4]]] ngram_indexes = [] for idx in range ( len ( cand_indexes )): ngram_index = [] for n in ngrams : ngram_index . append ( cand_indexes [ idx : idx + n ]) ngram_indexes . append ( ngram_index ) # \u8fd9\u91ccshuffle\u4e3a\u4e86\u4e4b\u540e\u968f\u673a\u91c7\u6837\u505a\u51c6\u5907\uff0c\u5148shuffle\uff0c\u518d\u62ff\u524dn\u4e2a\uff0c\u5c31\u76f8\u5f53\u4e8e\u968f\u673a\u91c7\u6837n\u4e2a np_rng . shuffle ( ngram_indexes ) ( masked_lms , masked_spans ) = ([], []) # covered_indexes\u4fdd\u5b58\u88abmask\u8fc7\u7684indexes covered_indexes = set () for cand_index_set in ngram_indexes : # \u5982\u679cmask\u6389\u7684\u5355\u8bcd\u6570\u91cf\u8d85\u8fc7\u9884\u671f\u5c31\u7ed3\u675fmask\u8fc7\u7a0b if len ( masked_lms ) >= num_to_predict : break if not cand_index_set : continue # Note(mingdachen): # Skip current piece if they are covered in lm masking or previous ngrams. # \u8fd9\u4e2a\u5faa\u73af\u786e\u5b9e\u6ca1\u770b\u61c2\u5728\u5e72\u5565 for index_set in cand_index_set [ 0 ]: for index in index_set : if index in covered_indexes : continue # \u6839\u636e\u4e4b\u524d\u7684pval\u9009\u62e9\u4e00\u4e2angrams\u7684\u503c\u6765\u9009\u62e9mask\u7684\u5355\u8bcd\u5e8f\u5217\u7684\u957f\u5ea6 if not geometric_dist : n = np_rng . choice ( ngrams [: len ( cand_index_set )], p = pvals [: len ( cand_index_set )] / pvals [: len ( cand_index_set )] . sum ( keepdims = True )) else : # Sampling \"n\" from the geometric distribution and clipping it to # the max_ngrams. Using p=0.2 default from the SpanBERT paper # https://arxiv.org/pdf/1907.10529.pdf (Sec 3.1) n = min ( np_rng . geometric ( 0.2 ), max_ngrams ) # \u628acand_index_set[0: n]\u7684\u6240\u6709\u5143\u7d20concat\u8d77\u6765 index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 # Note(mingdachen): # Repeatedly looking for a candidate that does not exceed the # maximum number of predictions by trying shorter ngrams. while len ( masked_lms ) + len ( index_set ) > num_to_predict : if n == 0 : break index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 # If adding a whole-word mask would exceed the maximum number of # predictions, then just skip this candidate. if len ( masked_lms ) + len ( index_set ) > num_to_predict : continue is_any_index_covered = False for index in index_set : if index in covered_indexes : is_any_index_covered = True break if is_any_index_covered : continue for index in index_set : covered_indexes . add ( index ) masked_token = None if masking_style == \"bert\" : # 80% of the time, replace with [MASK] if np_rng . random () < 0.8 : masked_token = mask_id else : # 10% of the time, keep original if np_rng . random () < 0.5 : masked_token = tokens [ index ] # 10% of the time, replace with random word else : masked_token = vocab_id_list [ np_rng . randint ( 0 , len ( vocab_id_list ))] elif masking_style == \"t5\" : masked_token = mask_id else : raise ValueError ( \"invalid value of masking style\" ) output_tokens [ index ] = masked_token masked_lms . append ( MaskedLmInstance ( index = index , label = tokens [ index ])) masked_spans . append ( MaskedLmInstance ( index = index_set , label = [ tokens [ index ] for index in index_set ])) assert len ( masked_lms ) <= num_to_predict np_rng . shuffle ( ngram_indexes ) select_indexes = set () if do_permutation : for cand_index_set in ngram_indexes : if len ( select_indexes ) >= num_to_predict : break if not cand_index_set : continue # Note(mingdachen): # Skip current piece if they are covered in lm masking or previous ngrams. for index_set in cand_index_set [ 0 ]: for index in index_set : if index in covered_indexes or index in select_indexes : continue n = np . random . choice ( ngrams [: len ( cand_index_set )], p = pvals [: len ( cand_index_set )] / pvals [: len ( cand_index_set )] . sum ( keepdims = True )) index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 while len ( select_indexes ) + len ( index_set ) > num_to_predict : if n == 0 : break index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 # If adding a whole-word mask would exceed the maximum number of # predictions, then just skip this candidate. if len ( select_indexes ) + len ( index_set ) > num_to_predict : continue is_any_index_covered = False for index in index_set : if index in covered_indexes or index in select_indexes : is_any_index_covered = True break if is_any_index_covered : continue for index in index_set : select_indexes . add ( index ) assert len ( select_indexes ) <= num_to_predict select_indexes = sorted ( select_indexes ) permute_indexes = list ( select_indexes ) np_rng . shuffle ( permute_indexes ) orig_token = list ( output_tokens ) for src_i , tgt_i in zip ( select_indexes , permute_indexes ): output_tokens [ src_i ] = orig_token [ tgt_i ] masked_lms . append ( MaskedLmInstance ( index = src_i , label = orig_token [ src_i ])) masked_lms = sorted ( masked_lms , key = lambda x : x . index ) # Sort the spans by the index of the first span masked_spans = sorted ( masked_spans , key = lambda x : x . index [ 0 ]) for p in masked_lms : masked_lm_positions . append ( p . index ) masked_lm_labels . append ( p . label ) return ( output_tokens , masked_lm_positions , masked_lm_labels , token_boundary , masked_spans )","title":"Note"},{"location":"note/#megatron-create_masked_lm_predictions","text":"\u4ee3\u7801\u5730\u5740\u5728 https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/dataset_utils.py#L181-L380 def create_masked_lm_predictions ( tokens , vocab_id_list , vocab_id_to_token_dict , masked_lm_prob , cls_id , sep_id , mask_id , max_predictions_per_seq , np_rng , max_ngrams = 3 , do_whole_word_mask = True , favor_longer_ngram = False , do_permutation = False , geometric_dist = False , masking_style = \"bert\" ): \"\"\"Creates the predictions for the masked LM objective. Note: Tokens here are vocab ids and not text tokens.\"\"\" cand_indexes = [] # Note(mingdachen): We create a list for recording if the piece is # the starting piece of current token, where 1 means true, so that # on-the-fly whole word masking is possible. # token_boundary\u662f\u4e00\u4e2a\u957f\u5ea6\u548ctokens\u76f8\u540c\u7684list\uff0c\u4fdd\u5b58\u7684\u503c\u53ea\u67091\u548c0 # 1\u8868\u793a\u5bf9\u5e94\u4f4d\u7f6e\u7684token\u662f\u4e00\u4e2a\u8bcd\u8bed\u7684\u5f00\u5934piece\uff0c0\u8868\u793a\u5bf9\u5e94\u4f4d\u7f6e\u7684token\u4e0d\u662f\u4e00\u4e2a\u8bcd\u8bed\u7684\u5f00\u5934\uff0c\u6bd4\u5982 ##ing\u8fd9\u6837 # \u4e4b\u6240\u4ee5\u8981\u8fd9\u4e48\u8bb0\u5f55\uff0c\u662f\u56e0\u4e3awhole word mask\u9700\u8981\u628a\u6574\u4e2aword\u8fdb\u884cmask # \u5982\u679c\u968f\u673a\u9009\u5230\u4e86\u67d0\u4e2aword piece\u8fdb\u884cmask\uff0c\u90a3\u4e48\u548c\u5b83\u5c5e\u4e8e\u540c\u4e00\u4e2a\u8bcd\u7684\u5176\u4ed6piece\u90fd\u8981mask token_boundary = [ 0 ] * len ( tokens ) # \u4e0b\u9762\u8fd9\u4e2a\u5faa\u73af\u7528\u6765\u4fee\u6539token_boundary\u8fd9\u4e2alist\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u503c for ( i , token ) in enumerate ( tokens ): if token == cls_id or token == sep_id : token_boundary [ i ] = 1 continue # Whole Word Masking means that if we mask all of the wordpieces # corresponding to an original word. # # Note that Whole Word Masking does *not* change the training code # at all -- we still predict each WordPiece independently, softmaxed # over the entire vocabulary. # is_start_piece\u7528\u6765\u5224\u65ad\u662f\u5426\u662f\u5f00\u5934\u7684word piece\uff0c\u5224\u65ad\u65b9\u6cd5\u5c31\u770btoken\u5bf9\u5e94\u7684word piece\u662f\u5426\u4ee5##\u5f00\u5934 if ( do_whole_word_mask and len ( cand_indexes ) >= 1 and not is_start_piece ( vocab_id_to_token_dict [ token ])): cand_indexes [ - 1 ] . append ( i ) else : cand_indexes . append ([ i ]) if is_start_piece ( vocab_id_to_token_dict [ token ]): token_boundary [ i ] = 1 # output_tokens\u662f\u8f93\u5165tokens\u7684\u4e00\u4e2a\u526f\u672c\uff0c\u7528\u6765\u8f93\u51famask\u4e4b\u540e\u7684token output_tokens = list ( tokens ) # \u4fdd\u5b58mask\u6389\u6570\u636e\u7684\u4f4d\u7f6e\u548c\u5bf9\u5e94\u7684label\uff08label\u5176\u5b9e\u5c31\u662ftoken_id\uff09 masked_lm_positions = [] masked_lm_labels = [] if masked_lm_prob == 0 : return ( output_tokens , masked_lm_positions , masked_lm_labels , token_boundary ) # \u8fd9\u91cc\u8ba1\u7b97\u8981mask\u7684\u6570\u91cf\uff0c\u6709\u4e00\u4e2amax_predictions_per_seq\u7528\u6765\u7ea6\u675f\u6700\u5927\u503c num_to_predict = min ( max_predictions_per_seq , max ( 1 , int ( round ( len ( tokens ) * masked_lm_prob )))) # ngrams\u4fdd\u5b58\u88abmask\u7684word\u5e8f\u5217\u6700\u5927\u957f\u5ea6\uff08\u4e0d\u662fword piece\uff09 # \u6bd4\u5982ngrams==3\uff0c\u90a3\u4e48\u6700\u591a\u53ef\u4ee5\u8fde\u7eedmask\u4e09\u4e2a\u5355\u8bcd ngrams = np . arange ( 1 , max_ngrams + 1 , dtype = np . int64 ) # \u8fd9\u91ccpval\u662f\u4e0d\u540c\u957f\u5ea6\u5355\u8bcd\u5e8f\u5217\u88abmask\u7684\u6982\u7387\uff0c\u6bd4\u5982ngrams==3\uff0c\u90a3\u4e48\u6982\u7387\u5c31\u662f(1, 1/2, 1/3)\uff0c\u5f52\u4e00\u5316\u5c31\u662f(6/11, 3/11, 2/11) if not geometric_dist : # Note(mingdachen): # By default, we set the probilities to favor shorter ngram sequences. pvals = 1. / np . arange ( 1 , max_ngrams + 1 ) pvals /= pvals . sum ( keepdims = True ) if favor_longer_ngram : pvals = pvals [:: - 1 ] # \u8fd9\u91ccngram_indexes\u4fdd\u5b58\u6240\u6709\u957f\u5ea6==[1, max_ngrams)\u7684\u5355\u8bcd\u5e8f\u5217 # \u6bd4\u5982tokens=[[1], [2, 3], [4]]\uff0cmax_ngrams==3\uff0c # \u90a3\u4e48ngram_indexes==[[[1]], [[1], [2, 3]], [[1], [2, 3], [4]], [[2, 3]], [[2, 3], [4]], [[4]]] ngram_indexes = [] for idx in range ( len ( cand_indexes )): ngram_index = [] for n in ngrams : ngram_index . append ( cand_indexes [ idx : idx + n ]) ngram_indexes . append ( ngram_index ) # \u8fd9\u91ccshuffle\u4e3a\u4e86\u4e4b\u540e\u968f\u673a\u91c7\u6837\u505a\u51c6\u5907\uff0c\u5148shuffle\uff0c\u518d\u62ff\u524dn\u4e2a\uff0c\u5c31\u76f8\u5f53\u4e8e\u968f\u673a\u91c7\u6837n\u4e2a np_rng . shuffle ( ngram_indexes ) ( masked_lms , masked_spans ) = ([], []) # covered_indexes\u4fdd\u5b58\u88abmask\u8fc7\u7684indexes covered_indexes = set () for cand_index_set in ngram_indexes : # \u5982\u679cmask\u6389\u7684\u5355\u8bcd\u6570\u91cf\u8d85\u8fc7\u9884\u671f\u5c31\u7ed3\u675fmask\u8fc7\u7a0b if len ( masked_lms ) >= num_to_predict : break if not cand_index_set : continue # Note(mingdachen): # Skip current piece if they are covered in lm masking or previous ngrams. # \u8fd9\u4e2a\u5faa\u73af\u786e\u5b9e\u6ca1\u770b\u61c2\u5728\u5e72\u5565 for index_set in cand_index_set [ 0 ]: for index in index_set : if index in covered_indexes : continue # \u6839\u636e\u4e4b\u524d\u7684pval\u9009\u62e9\u4e00\u4e2angrams\u7684\u503c\u6765\u9009\u62e9mask\u7684\u5355\u8bcd\u5e8f\u5217\u7684\u957f\u5ea6 if not geometric_dist : n = np_rng . choice ( ngrams [: len ( cand_index_set )], p = pvals [: len ( cand_index_set )] / pvals [: len ( cand_index_set )] . sum ( keepdims = True )) else : # Sampling \"n\" from the geometric distribution and clipping it to # the max_ngrams. Using p=0.2 default from the SpanBERT paper # https://arxiv.org/pdf/1907.10529.pdf (Sec 3.1) n = min ( np_rng . geometric ( 0.2 ), max_ngrams ) # \u628acand_index_set[0: n]\u7684\u6240\u6709\u5143\u7d20concat\u8d77\u6765 index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 # Note(mingdachen): # Repeatedly looking for a candidate that does not exceed the # maximum number of predictions by trying shorter ngrams. while len ( masked_lms ) + len ( index_set ) > num_to_predict : if n == 0 : break index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 # If adding a whole-word mask would exceed the maximum number of # predictions, then just skip this candidate. if len ( masked_lms ) + len ( index_set ) > num_to_predict : continue is_any_index_covered = False for index in index_set : if index in covered_indexes : is_any_index_covered = True break if is_any_index_covered : continue for index in index_set : covered_indexes . add ( index ) masked_token = None if masking_style == \"bert\" : # 80% of the time, replace with [MASK] if np_rng . random () < 0.8 : masked_token = mask_id else : # 10% of the time, keep original if np_rng . random () < 0.5 : masked_token = tokens [ index ] # 10% of the time, replace with random word else : masked_token = vocab_id_list [ np_rng . randint ( 0 , len ( vocab_id_list ))] elif masking_style == \"t5\" : masked_token = mask_id else : raise ValueError ( \"invalid value of masking style\" ) output_tokens [ index ] = masked_token masked_lms . append ( MaskedLmInstance ( index = index , label = tokens [ index ])) masked_spans . append ( MaskedLmInstance ( index = index_set , label = [ tokens [ index ] for index in index_set ])) assert len ( masked_lms ) <= num_to_predict np_rng . shuffle ( ngram_indexes ) select_indexes = set () if do_permutation : for cand_index_set in ngram_indexes : if len ( select_indexes ) >= num_to_predict : break if not cand_index_set : continue # Note(mingdachen): # Skip current piece if they are covered in lm masking or previous ngrams. for index_set in cand_index_set [ 0 ]: for index in index_set : if index in covered_indexes or index in select_indexes : continue n = np . random . choice ( ngrams [: len ( cand_index_set )], p = pvals [: len ( cand_index_set )] / pvals [: len ( cand_index_set )] . sum ( keepdims = True )) index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 while len ( select_indexes ) + len ( index_set ) > num_to_predict : if n == 0 : break index_set = sum ( cand_index_set [ n - 1 ], []) n -= 1 # If adding a whole-word mask would exceed the maximum number of # predictions, then just skip this candidate. if len ( select_indexes ) + len ( index_set ) > num_to_predict : continue is_any_index_covered = False for index in index_set : if index in covered_indexes or index in select_indexes : is_any_index_covered = True break if is_any_index_covered : continue for index in index_set : select_indexes . add ( index ) assert len ( select_indexes ) <= num_to_predict select_indexes = sorted ( select_indexes ) permute_indexes = list ( select_indexes ) np_rng . shuffle ( permute_indexes ) orig_token = list ( output_tokens ) for src_i , tgt_i in zip ( select_indexes , permute_indexes ): output_tokens [ src_i ] = orig_token [ tgt_i ] masked_lms . append ( MaskedLmInstance ( index = src_i , label = orig_token [ src_i ])) masked_lms = sorted ( masked_lms , key = lambda x : x . index ) # Sort the spans by the index of the first span masked_spans = sorted ( masked_spans , key = lambda x : x . index [ 0 ]) for p in masked_lms : masked_lm_positions . append ( p . index ) masked_lm_labels . append ( p . label ) return ( output_tokens , masked_lm_positions , masked_lm_labels , token_boundary , masked_spans )","title":"Megatron \u6e90\u7801\u9605\u8bfb\u7b14\u8bb0 \u2014\u2014 create_masked_lm_predictions"},{"location":"pad_and_convert_to_numpy/","text":"Megatron\u6e90\u7801\u9605\u8bfb\u7b14\u8bb0\u2014\u2014 pad_and_convert_to_numpy https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/t5_dataset.py#L160-L230 def pad_and_convert_to_numpy ( tokens , masked_positions , masked_labels , pad_id , max_seq_length , max_seq_length_dec , masked_spans = None , bos_id = None , eos_id = None , sentinel_tokens = None ): \"\"\"Pad sequences and convert them to numpy.\"\"\" # \u8fd9\u4e2a\u90e8\u5206\u6ca1\u6709\u7406\u89e3\u4e3a\u4ec0\u4e48\u8981\u7528sentinel_tokens sentinel_tokens = collections . deque ( sentinel_tokens ) t5_input = [] # decoder\u5f00\u5934\u4e00\u4e2abegin of sequence ( t5_decoder_in , t5_decoder_out ) = ([ bos_id ], []) ( start_index , end_index ) = ( 0 , None ) # masked_spans\u662f\u4e00\u4e2a[masked_indexes, masked_labels]\u7684\u96c6\u5408 for span in masked_spans : flag = sentinel_tokens . popleft () # Append the same tokens in decoder input and output t5_decoder_in . append ( flag ) t5_decoder_in . extend ( span . label ) t5_decoder_out . append ( flag ) t5_decoder_out . extend ( span . label ) # \u8fd9\u91ccend_index\u662f\u5f53\u524dspan\u4e2d\u88abmasked\u7684\u7b2c\u4e00\u4e2atoken\u7684index end_index = span . index [ 0 ] t5_input . extend ( tokens [ start_index : end_index ]) # \u6211\u4e5f\u4e0d\u77e5\u9053\u8fd9\u91ccflag\u662f\u5565\u610f\u601d t5_input . append ( flag ) # the next start index is the token after the last span token start_index = span . index [ - 1 ] + 1 # Add <eos> token to the t5_decoder_out t5_decoder_out . append ( eos_id ) # Add the remaining tokens to the t5 input t5_input . extend ( tokens [ start_index :]) # assert (len(t5_input) - len(masked_spans)) + \\ # (len(t5_decoder_in) - (len(masked_spans) + 1)) == len(tokens) # Some checks. # Encoder-side padding mask. num_tokens = len ( t5_input ) padding_length = max_seq_length - num_tokens assert padding_length >= 0 assert len ( masked_positions ) == len ( masked_labels ) # Tokens.. filler = [ pad_id ] * padding_length tokens_enc = np . array ( t5_input + filler , dtype = np . int64 ) # Decoder-side padding mask. num_tokens_dec = len ( t5_decoder_in ) padding_length_dec = max_seq_length_dec - num_tokens_dec assert padding_length_dec >= 0 filler_dec = [ pad_id ] * padding_length_dec tokens_dec_in = np . array ( t5_decoder_in + filler_dec , dtype = np . int64 ) # Create attention masks enc_mask = make_attention_mask ( tokens_enc , tokens_enc ) enc_dec_mask = make_attention_mask ( tokens_dec_in , tokens_enc ) dec_mask = make_attention_mask ( tokens_dec_in , tokens_dec_in ) dec_mask = dec_mask * make_history_mask ( tokens_dec_in ) # \u8fd9\u91cc\u7528masked\u6389\u7684tokens\u5f53\u6210labels\uff0c\u7528\u4f5c\u9884\u8bad\u7ec3 # Labels mask. labels = t5_decoder_out + ([ - 1 ] * padding_length_dec ) labels = np . array ( labels , dtype = np . int64 ) # Loss mask loss_mask = ([ 1 ] * num_tokens_dec ) + ([ 0 ] * padding_length_dec ) loss_mask = np . array ( loss_mask , dtype = np . int64 ) return tokens_enc , tokens_dec_in , labels , enc_mask , \\ dec_mask , enc_dec_mask , loss_mask","title":"Pad and convert to numpy"},{"location":"pad_and_convert_to_numpy/#megatronpad_and_convert_to_numpy","text":"https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/t5_dataset.py#L160-L230 def pad_and_convert_to_numpy ( tokens , masked_positions , masked_labels , pad_id , max_seq_length , max_seq_length_dec , masked_spans = None , bos_id = None , eos_id = None , sentinel_tokens = None ): \"\"\"Pad sequences and convert them to numpy.\"\"\" # \u8fd9\u4e2a\u90e8\u5206\u6ca1\u6709\u7406\u89e3\u4e3a\u4ec0\u4e48\u8981\u7528sentinel_tokens sentinel_tokens = collections . deque ( sentinel_tokens ) t5_input = [] # decoder\u5f00\u5934\u4e00\u4e2abegin of sequence ( t5_decoder_in , t5_decoder_out ) = ([ bos_id ], []) ( start_index , end_index ) = ( 0 , None ) # masked_spans\u662f\u4e00\u4e2a[masked_indexes, masked_labels]\u7684\u96c6\u5408 for span in masked_spans : flag = sentinel_tokens . popleft () # Append the same tokens in decoder input and output t5_decoder_in . append ( flag ) t5_decoder_in . extend ( span . label ) t5_decoder_out . append ( flag ) t5_decoder_out . extend ( span . label ) # \u8fd9\u91ccend_index\u662f\u5f53\u524dspan\u4e2d\u88abmasked\u7684\u7b2c\u4e00\u4e2atoken\u7684index end_index = span . index [ 0 ] t5_input . extend ( tokens [ start_index : end_index ]) # \u6211\u4e5f\u4e0d\u77e5\u9053\u8fd9\u91ccflag\u662f\u5565\u610f\u601d t5_input . append ( flag ) # the next start index is the token after the last span token start_index = span . index [ - 1 ] + 1 # Add <eos> token to the t5_decoder_out t5_decoder_out . append ( eos_id ) # Add the remaining tokens to the t5 input t5_input . extend ( tokens [ start_index :]) # assert (len(t5_input) - len(masked_spans)) + \\ # (len(t5_decoder_in) - (len(masked_spans) + 1)) == len(tokens) # Some checks. # Encoder-side padding mask. num_tokens = len ( t5_input ) padding_length = max_seq_length - num_tokens assert padding_length >= 0 assert len ( masked_positions ) == len ( masked_labels ) # Tokens.. filler = [ pad_id ] * padding_length tokens_enc = np . array ( t5_input + filler , dtype = np . int64 ) # Decoder-side padding mask. num_tokens_dec = len ( t5_decoder_in ) padding_length_dec = max_seq_length_dec - num_tokens_dec assert padding_length_dec >= 0 filler_dec = [ pad_id ] * padding_length_dec tokens_dec_in = np . array ( t5_decoder_in + filler_dec , dtype = np . int64 ) # Create attention masks enc_mask = make_attention_mask ( tokens_enc , tokens_enc ) enc_dec_mask = make_attention_mask ( tokens_dec_in , tokens_enc ) dec_mask = make_attention_mask ( tokens_dec_in , tokens_dec_in ) dec_mask = dec_mask * make_history_mask ( tokens_dec_in ) # \u8fd9\u91cc\u7528masked\u6389\u7684tokens\u5f53\u6210labels\uff0c\u7528\u4f5c\u9884\u8bad\u7ec3 # Labels mask. labels = t5_decoder_out + ([ - 1 ] * padding_length_dec ) labels = np . array ( labels , dtype = np . int64 ) # Loss mask loss_mask = ([ 1 ] * num_tokens_dec ) + ([ 0 ] * padding_length_dec ) loss_mask = np . array ( loss_mask , dtype = np . int64 ) return tokens_enc , tokens_dec_in , labels , enc_mask , \\ dec_mask , enc_dec_mask , loss_mask","title":"Megatron\u6e90\u7801\u9605\u8bfb\u7b14\u8bb0\u2014\u2014pad_and_convert_to_numpy"}]}