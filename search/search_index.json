{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"test"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"note/","text":"Megatron \u6e90\u7801\u9605\u8bfb\u7b14\u8bb0 \u2014\u2014 create_masked_lm_predictions \u4ee3\u7801\u5730\u5740\u5728 https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/dataset_utils.py#L181-L380 def create_masked_lm_predictions(tokens, vocab_id_list, vocab_id_to_token_dict, masked_lm_prob, cls_id, sep_id, mask_id, max_predictions_per_seq, np_rng, max_ngrams=3, do_whole_word_mask=True, favor_longer_ngram=False, do_permutation=False, geometric_dist=False, masking_style=\"bert\"): \"\"\"Creates the predictions for the masked LM objective. Note: Tokens here are vocab ids and not text tokens.\"\"\" cand_indexes = [] # Note(mingdachen): We create a list for recording if the piece is # the starting piece of current token, where 1 means true, so that # on-the-fly whole word masking is possible. # token_boundary\u662f\u4e00\u4e2a\u957f\u5ea6\u548ctokens\u76f8\u540c\u7684list\uff0c\u4fdd\u5b58\u7684\u503c\u53ea\u67091\u548c0 # 1\u8868\u793a\u5bf9\u5e94\u4f4d\u7f6e\u7684token\u662f\u4e00\u4e2a\u8bcd\u8bed\u7684\u5f00\u5934piece\uff0c0\u8868\u793a\u5bf9\u5e94\u4f4d\u7f6e\u7684token\u4e0d\u662f\u4e00\u4e2a\u8bcd\u8bed\u7684\u5f00\u5934\uff0c\u6bd4\u5982 ##ing\u8fd9\u6837 # \u4e4b\u6240\u4ee5\u8981\u8fd9\u4e48\u8bb0\u5f55\uff0c\u662f\u56e0\u4e3awhole word mask\u9700\u8981\u628a\u6574\u4e2aword\u8fdb\u884cmask # \u5982\u679c\u968f\u673a\u9009\u5230\u4e86\u67d0\u4e2aword piece\u8fdb\u884cmask\uff0c\u90a3\u4e48\u548c\u5b83\u5c5e\u4e8e\u540c\u4e00\u4e2a\u8bcd\u7684\u5176\u4ed6piece\u90fd\u8981mask token_boundary = [0] * len(tokens) # \u4e0b\u9762\u8fd9\u4e2a\u5faa\u73af\u7528\u6765\u4fee\u6539token_boundary\u8fd9\u4e2alist\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u503c for (i, token) in enumerate(tokens): if token == cls_id or token == sep_id: token_boundary[i] = 1 continue # Whole Word Masking means that if we mask all of the wordpieces # corresponding to an original word. # # Note that Whole Word Masking does *not* change the training code # at all -- we still predict each WordPiece independently, softmaxed # over the entire vocabulary. # is_start_piece\u7528\u6765\u5224\u65ad\u662f\u5426\u662f\u5f00\u5934\u7684word piece\uff0c\u5224\u65ad\u65b9\u6cd5\u5c31\u770btoken\u5bf9\u5e94\u7684word piece\u662f\u5426\u4ee5##\u5f00\u5934 if (do_whole_word_mask and len(cand_indexes) >= 1 and not is_start_piece(vocab_id_to_token_dict[token])): cand_indexes[-1].append(i) else: cand_indexes.append([i]) if is_start_piece(vocab_id_to_token_dict[token]): token_boundary[i] = 1 # output_tokens\u662f\u8f93\u5165tokens\u7684\u4e00\u4e2a\u526f\u672c\uff0c\u7528\u6765\u8f93\u51famask\u4e4b\u540e\u7684token output_tokens = list(tokens) # \u4fdd\u5b58mask\u6389\u6570\u636e\u7684\u4f4d\u7f6e\u548c\u5bf9\u5e94\u7684label\uff08label\u5176\u5b9e\u5c31\u662ftoken_id\uff09 masked_lm_positions = [] masked_lm_labels = [] if masked_lm_prob == 0: return (output_tokens, masked_lm_positions, masked_lm_labels, token_boundary) # \u8fd9\u91cc\u8ba1\u7b97\u8981mask\u7684\u6570\u91cf\uff0c\u6709\u4e00\u4e2amax_predictions_per_seq\u7528\u6765\u7ea6\u675f\u6700\u5927\u503c num_to_predict = min(max_predictions_per_seq, max(1, int(round(len(tokens) * masked_lm_prob)))) # ngrams\u4fdd\u5b58\u88abmask\u7684word\u5e8f\u5217\u6700\u5927\u957f\u5ea6\uff08\u4e0d\u662fword piece\uff09 # \u6bd4\u5982ngrams==3\uff0c\u90a3\u4e48\u6700\u591a\u53ef\u4ee5\u8fde\u7eedmask\u4e09\u4e2a\u5355\u8bcd ngrams = np.arange(1, max_ngrams + 1, dtype=np.int64) # \u8fd9\u91ccpval\u662f\u4e0d\u540c\u957f\u5ea6\u5355\u8bcd\u5e8f\u5217\u88abmask\u7684\u6982\u7387\uff0c\u6bd4\u5982ngrams==3\uff0c\u90a3\u4e48\u6982\u7387\u5c31\u662f(1, 1/2, 1/3)\uff0c\u5f52\u4e00\u5316\u5c31\u662f(6/11, 3/11, 2/11) if not geometric_dist: # Note(mingdachen): # By default, we set the probilities to favor shorter ngram sequences. pvals = 1. / np.arange(1, max_ngrams + 1) pvals /= pvals.sum(keepdims=True) if favor_longer_ngram: pvals = pvals[::-1] # \u8fd9\u91ccngram_indexes\u4fdd\u5b58\u6240\u6709\u957f\u5ea6==[1, max_ngrams)\u7684\u5355\u8bcd\u5e8f\u5217 # \u6bd4\u5982tokens=[[1], [2, 3], [4]]\uff0cmax_ngrams==3\uff0c # \u90a3\u4e48ngram_indexes==[[[1]], [[1], [2, 3]], [[1], [2, 3], [4]], [[2, 3]], [[2, 3], [4]], [[4]]] ngram_indexes = [] for idx in range(len(cand_indexes)): ngram_index = [] for n in ngrams: ngram_index.append(cand_indexes[idx:idx + n]) ngram_indexes.append(ngram_index) # \u8fd9\u91ccshuffle\u4e3a\u4e86\u4e4b\u540e\u968f\u673a\u91c7\u6837\u505a\u51c6\u5907\uff0c\u5148shuffle\uff0c\u518d\u62ff\u524dn\u4e2a\uff0c\u5c31\u76f8\u5f53\u4e8e\u968f\u673a\u91c7\u6837n\u4e2a np_rng.shuffle(ngram_indexes) (masked_lms, masked_spans) = ([], []) # covered_indexes\u4fdd\u5b58\u88abmask\u8fc7\u7684indexes covered_indexes = set() for cand_index_set in ngram_indexes: # \u5982\u679cmask\u6389\u7684\u5355\u8bcd\u6570\u91cf\u8d85\u8fc7\u9884\u671f\u5c31\u7ed3\u675fmask\u8fc7\u7a0b if len(masked_lms) >= num_to_predict: break if not cand_index_set: continue # Note(mingdachen): # Skip current piece if they are covered in lm masking or previous ngrams. # \u8fd9\u4e2a\u5faa\u73af\u786e\u5b9e\u6ca1\u770b\u61c2\u5728\u5e72\u5565 for index_set in cand_index_set[0]: for index in index_set: if index in covered_indexes: continue # \u6839\u636e\u4e4b\u524d\u7684pval\u9009\u62e9\u4e00\u4e2angrams\u7684\u503c\u6765\u9009\u62e9mask\u7684\u5355\u8bcd\u5e8f\u5217\u7684\u957f\u5ea6 if not geometric_dist: n = np_rng.choice(ngrams[:len(cand_index_set)], p=pvals[:len(cand_index_set)] / pvals[:len(cand_index_set)].sum(keepdims=True)) else: # Sampling \"n\" from the geometric distribution and clipping it to # the max_ngrams. Using p=0.2 default from the SpanBERT paper # https://arxiv.org/pdf/1907.10529.pdf (Sec 3.1) n = min(np_rng.geometric(0.2), max_ngrams) # \u628acand_index_set[0: n]\u7684\u6240\u6709\u5143\u7d20concat\u8d77\u6765 index_set = sum(cand_index_set[n - 1], []) n -= 1 # Note(mingdachen): # Repeatedly looking for a candidate that does not exceed the # maximum number of predictions by trying shorter ngrams. while len(masked_lms) + len(index_set) > num_to_predict: if n == 0: break index_set = sum(cand_index_set[n - 1], []) n -= 1 # If adding a whole-word mask would exceed the maximum number of # predictions, then just skip this candidate. if len(masked_lms) + len(index_set) > num_to_predict: continue is_any_index_covered = False for index in index_set: if index in covered_indexes: is_any_index_covered = True break if is_any_index_covered: continue for index in index_set: covered_indexes.add(index) masked_token = None if masking_style == \"bert\": # 80% of the time, replace with [MASK] if np_rng.random() < 0.8: masked_token = mask_id else: # 10% of the time, keep original if np_rng.random() < 0.5: masked_token = tokens[index] # 10% of the time, replace with random word else: masked_token = vocab_id_list[np_rng.randint(0, len(vocab_id_list))] elif masking_style == \"t5\": masked_token = mask_id else: raise ValueError(\"invalid value of masking style\") output_tokens[index] = masked_token masked_lms.append(MaskedLmInstance(index=index, label=tokens[index])) masked_spans.append(MaskedLmInstance( index=index_set, label=[tokens[index] for index in index_set])) assert len(masked_lms) <= num_to_predict np_rng.shuffle(ngram_indexes) select_indexes = set() if do_permutation: for cand_index_set in ngram_indexes: if len(select_indexes) >= num_to_predict: break if not cand_index_set: continue # Note(mingdachen): # Skip current piece if they are covered in lm masking or previous ngrams. for index_set in cand_index_set[0]: for index in index_set: if index in covered_indexes or index in select_indexes: continue n = np.random.choice(ngrams[:len(cand_index_set)], p=pvals[:len(cand_index_set)] / pvals[:len(cand_index_set)].sum(keepdims=True)) index_set = sum(cand_index_set[n - 1], []) n -= 1 while len(select_indexes) + len(index_set) > num_to_predict: if n == 0: break index_set = sum(cand_index_set[n - 1], []) n -= 1 # If adding a whole-word mask would exceed the maximum number of # predictions, then just skip this candidate. if len(select_indexes) + len(index_set) > num_to_predict: continue is_any_index_covered = False for index in index_set: if index in covered_indexes or index in select_indexes: is_any_index_covered = True break if is_any_index_covered: continue for index in index_set: select_indexes.add(index) assert len(select_indexes) <= num_to_predict select_indexes = sorted(select_indexes) permute_indexes = list(select_indexes) np_rng.shuffle(permute_indexes) orig_token = list(output_tokens) for src_i, tgt_i in zip(select_indexes, permute_indexes): output_tokens[src_i] = orig_token[tgt_i] masked_lms.append(MaskedLmInstance(index=src_i, label=orig_token[src_i])) masked_lms = sorted(masked_lms, key=lambda x: x.index) # Sort the spans by the index of the first span masked_spans = sorted(masked_spans, key=lambda x: x.index[0]) for p in masked_lms: masked_lm_positions.append(p.index) masked_lm_labels.append(p.label) return (output_tokens, masked_lm_positions, masked_lm_labels, token_boundary, masked_spans)","title":"\u6e90\u7801\u9605\u8bfb\u7b14\u8bb0"},{"location":"note/#megatron-create_masked_lm_predictions","text":"\u4ee3\u7801\u5730\u5740\u5728 https://github.com/NVIDIA/Megatron-LM/blob/e156d2fea7fc5c98e645f7742eb86b643956d840/megatron/data/dataset_utils.py#L181-L380 def create_masked_lm_predictions(tokens, vocab_id_list, vocab_id_to_token_dict, masked_lm_prob, cls_id, sep_id, mask_id, max_predictions_per_seq, np_rng, max_ngrams=3, do_whole_word_mask=True, favor_longer_ngram=False, do_permutation=False, geometric_dist=False, masking_style=\"bert\"): \"\"\"Creates the predictions for the masked LM objective. Note: Tokens here are vocab ids and not text tokens.\"\"\" cand_indexes = [] # Note(mingdachen): We create a list for recording if the piece is # the starting piece of current token, where 1 means true, so that # on-the-fly whole word masking is possible. # token_boundary\u662f\u4e00\u4e2a\u957f\u5ea6\u548ctokens\u76f8\u540c\u7684list\uff0c\u4fdd\u5b58\u7684\u503c\u53ea\u67091\u548c0 # 1\u8868\u793a\u5bf9\u5e94\u4f4d\u7f6e\u7684token\u662f\u4e00\u4e2a\u8bcd\u8bed\u7684\u5f00\u5934piece\uff0c0\u8868\u793a\u5bf9\u5e94\u4f4d\u7f6e\u7684token\u4e0d\u662f\u4e00\u4e2a\u8bcd\u8bed\u7684\u5f00\u5934\uff0c\u6bd4\u5982 ##ing\u8fd9\u6837 # \u4e4b\u6240\u4ee5\u8981\u8fd9\u4e48\u8bb0\u5f55\uff0c\u662f\u56e0\u4e3awhole word mask\u9700\u8981\u628a\u6574\u4e2aword\u8fdb\u884cmask # \u5982\u679c\u968f\u673a\u9009\u5230\u4e86\u67d0\u4e2aword piece\u8fdb\u884cmask\uff0c\u90a3\u4e48\u548c\u5b83\u5c5e\u4e8e\u540c\u4e00\u4e2a\u8bcd\u7684\u5176\u4ed6piece\u90fd\u8981mask token_boundary = [0] * len(tokens) # \u4e0b\u9762\u8fd9\u4e2a\u5faa\u73af\u7528\u6765\u4fee\u6539token_boundary\u8fd9\u4e2alist\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u503c for (i, token) in enumerate(tokens): if token == cls_id or token == sep_id: token_boundary[i] = 1 continue # Whole Word Masking means that if we mask all of the wordpieces # corresponding to an original word. # # Note that Whole Word Masking does *not* change the training code # at all -- we still predict each WordPiece independently, softmaxed # over the entire vocabulary. # is_start_piece\u7528\u6765\u5224\u65ad\u662f\u5426\u662f\u5f00\u5934\u7684word piece\uff0c\u5224\u65ad\u65b9\u6cd5\u5c31\u770btoken\u5bf9\u5e94\u7684word piece\u662f\u5426\u4ee5##\u5f00\u5934 if (do_whole_word_mask and len(cand_indexes) >= 1 and not is_start_piece(vocab_id_to_token_dict[token])): cand_indexes[-1].append(i) else: cand_indexes.append([i]) if is_start_piece(vocab_id_to_token_dict[token]): token_boundary[i] = 1 # output_tokens\u662f\u8f93\u5165tokens\u7684\u4e00\u4e2a\u526f\u672c\uff0c\u7528\u6765\u8f93\u51famask\u4e4b\u540e\u7684token output_tokens = list(tokens) # \u4fdd\u5b58mask\u6389\u6570\u636e\u7684\u4f4d\u7f6e\u548c\u5bf9\u5e94\u7684label\uff08label\u5176\u5b9e\u5c31\u662ftoken_id\uff09 masked_lm_positions = [] masked_lm_labels = [] if masked_lm_prob == 0: return (output_tokens, masked_lm_positions, masked_lm_labels, token_boundary) # \u8fd9\u91cc\u8ba1\u7b97\u8981mask\u7684\u6570\u91cf\uff0c\u6709\u4e00\u4e2amax_predictions_per_seq\u7528\u6765\u7ea6\u675f\u6700\u5927\u503c num_to_predict = min(max_predictions_per_seq, max(1, int(round(len(tokens) * masked_lm_prob)))) # ngrams\u4fdd\u5b58\u88abmask\u7684word\u5e8f\u5217\u6700\u5927\u957f\u5ea6\uff08\u4e0d\u662fword piece\uff09 # \u6bd4\u5982ngrams==3\uff0c\u90a3\u4e48\u6700\u591a\u53ef\u4ee5\u8fde\u7eedmask\u4e09\u4e2a\u5355\u8bcd ngrams = np.arange(1, max_ngrams + 1, dtype=np.int64) # \u8fd9\u91ccpval\u662f\u4e0d\u540c\u957f\u5ea6\u5355\u8bcd\u5e8f\u5217\u88abmask\u7684\u6982\u7387\uff0c\u6bd4\u5982ngrams==3\uff0c\u90a3\u4e48\u6982\u7387\u5c31\u662f(1, 1/2, 1/3)\uff0c\u5f52\u4e00\u5316\u5c31\u662f(6/11, 3/11, 2/11) if not geometric_dist: # Note(mingdachen): # By default, we set the probilities to favor shorter ngram sequences. pvals = 1. / np.arange(1, max_ngrams + 1) pvals /= pvals.sum(keepdims=True) if favor_longer_ngram: pvals = pvals[::-1] # \u8fd9\u91ccngram_indexes\u4fdd\u5b58\u6240\u6709\u957f\u5ea6==[1, max_ngrams)\u7684\u5355\u8bcd\u5e8f\u5217 # \u6bd4\u5982tokens=[[1], [2, 3], [4]]\uff0cmax_ngrams==3\uff0c # \u90a3\u4e48ngram_indexes==[[[1]], [[1], [2, 3]], [[1], [2, 3], [4]], [[2, 3]], [[2, 3], [4]], [[4]]] ngram_indexes = [] for idx in range(len(cand_indexes)): ngram_index = [] for n in ngrams: ngram_index.append(cand_indexes[idx:idx + n]) ngram_indexes.append(ngram_index) # \u8fd9\u91ccshuffle\u4e3a\u4e86\u4e4b\u540e\u968f\u673a\u91c7\u6837\u505a\u51c6\u5907\uff0c\u5148shuffle\uff0c\u518d\u62ff\u524dn\u4e2a\uff0c\u5c31\u76f8\u5f53\u4e8e\u968f\u673a\u91c7\u6837n\u4e2a np_rng.shuffle(ngram_indexes) (masked_lms, masked_spans) = ([], []) # covered_indexes\u4fdd\u5b58\u88abmask\u8fc7\u7684indexes covered_indexes = set() for cand_index_set in ngram_indexes: # \u5982\u679cmask\u6389\u7684\u5355\u8bcd\u6570\u91cf\u8d85\u8fc7\u9884\u671f\u5c31\u7ed3\u675fmask\u8fc7\u7a0b if len(masked_lms) >= num_to_predict: break if not cand_index_set: continue # Note(mingdachen): # Skip current piece if they are covered in lm masking or previous ngrams. # \u8fd9\u4e2a\u5faa\u73af\u786e\u5b9e\u6ca1\u770b\u61c2\u5728\u5e72\u5565 for index_set in cand_index_set[0]: for index in index_set: if index in covered_indexes: continue # \u6839\u636e\u4e4b\u524d\u7684pval\u9009\u62e9\u4e00\u4e2angrams\u7684\u503c\u6765\u9009\u62e9mask\u7684\u5355\u8bcd\u5e8f\u5217\u7684\u957f\u5ea6 if not geometric_dist: n = np_rng.choice(ngrams[:len(cand_index_set)], p=pvals[:len(cand_index_set)] / pvals[:len(cand_index_set)].sum(keepdims=True)) else: # Sampling \"n\" from the geometric distribution and clipping it to # the max_ngrams. Using p=0.2 default from the SpanBERT paper # https://arxiv.org/pdf/1907.10529.pdf (Sec 3.1) n = min(np_rng.geometric(0.2), max_ngrams) # \u628acand_index_set[0: n]\u7684\u6240\u6709\u5143\u7d20concat\u8d77\u6765 index_set = sum(cand_index_set[n - 1], []) n -= 1 # Note(mingdachen): # Repeatedly looking for a candidate that does not exceed the # maximum number of predictions by trying shorter ngrams. while len(masked_lms) + len(index_set) > num_to_predict: if n == 0: break index_set = sum(cand_index_set[n - 1], []) n -= 1 # If adding a whole-word mask would exceed the maximum number of # predictions, then just skip this candidate. if len(masked_lms) + len(index_set) > num_to_predict: continue is_any_index_covered = False for index in index_set: if index in covered_indexes: is_any_index_covered = True break if is_any_index_covered: continue for index in index_set: covered_indexes.add(index) masked_token = None if masking_style == \"bert\": # 80% of the time, replace with [MASK] if np_rng.random() < 0.8: masked_token = mask_id else: # 10% of the time, keep original if np_rng.random() < 0.5: masked_token = tokens[index] # 10% of the time, replace with random word else: masked_token = vocab_id_list[np_rng.randint(0, len(vocab_id_list))] elif masking_style == \"t5\": masked_token = mask_id else: raise ValueError(\"invalid value of masking style\") output_tokens[index] = masked_token masked_lms.append(MaskedLmInstance(index=index, label=tokens[index])) masked_spans.append(MaskedLmInstance( index=index_set, label=[tokens[index] for index in index_set])) assert len(masked_lms) <= num_to_predict np_rng.shuffle(ngram_indexes) select_indexes = set() if do_permutation: for cand_index_set in ngram_indexes: if len(select_indexes) >= num_to_predict: break if not cand_index_set: continue # Note(mingdachen): # Skip current piece if they are covered in lm masking or previous ngrams. for index_set in cand_index_set[0]: for index in index_set: if index in covered_indexes or index in select_indexes: continue n = np.random.choice(ngrams[:len(cand_index_set)], p=pvals[:len(cand_index_set)] / pvals[:len(cand_index_set)].sum(keepdims=True)) index_set = sum(cand_index_set[n - 1], []) n -= 1 while len(select_indexes) + len(index_set) > num_to_predict: if n == 0: break index_set = sum(cand_index_set[n - 1], []) n -= 1 # If adding a whole-word mask would exceed the maximum number of # predictions, then just skip this candidate. if len(select_indexes) + len(index_set) > num_to_predict: continue is_any_index_covered = False for index in index_set: if index in covered_indexes or index in select_indexes: is_any_index_covered = True break if is_any_index_covered: continue for index in index_set: select_indexes.add(index) assert len(select_indexes) <= num_to_predict select_indexes = sorted(select_indexes) permute_indexes = list(select_indexes) np_rng.shuffle(permute_indexes) orig_token = list(output_tokens) for src_i, tgt_i in zip(select_indexes, permute_indexes): output_tokens[src_i] = orig_token[tgt_i] masked_lms.append(MaskedLmInstance(index=src_i, label=orig_token[src_i])) masked_lms = sorted(masked_lms, key=lambda x: x.index) # Sort the spans by the index of the first span masked_spans = sorted(masked_spans, key=lambda x: x.index[0]) for p in masked_lms: masked_lm_positions.append(p.index) masked_lm_labels.append(p.label) return (output_tokens, masked_lm_positions, masked_lm_labels, token_boundary, masked_spans)","title":"Megatron \u6e90\u7801\u9605\u8bfb\u7b14\u8bb0 \u2014\u2014 create_masked_lm_predictions"}]}